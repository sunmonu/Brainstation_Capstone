{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Bx0jc5jRq51"
      },
      "outputs": [],
      "source": [
        "#install all packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVWknN2iZB77",
        "outputId": "407f3a78-d634-4f2d-a848-acb4b9b9e5ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m756.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.58.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.11-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.2.0)\n",
            "Building wheels for collected packages: umap-learn\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.5-py3-none-any.whl size=86832 sha256=0ecdb8e6b0a6035b4c868c1b7b48a25f087b42971a738436fc82f51c0dd00024\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/70/07/428d2b58660a1a3b431db59b806a10da736612ebbc66c1bcc5\n",
            "Successfully built umap-learn\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.11 umap-learn-0.5.5\n"
          ]
        }
      ],
      "source": [
        "%pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtscMDSJZSi7"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE #TSNE is t-stochastic neighbor embeddings\n",
        "from umap.umap_ import UMAP\n",
        "from mpl_toolkits.mplot3d import axes3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9gDgiWPXpJa"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nmztKQcL92H"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx9SWVCEvmDQ"
      },
      "source": [
        "## Intro - Strategic Store Expansion Using Demographic Data\n",
        "\n",
        "In this project, we aim to uncover insights into the relationship between household debt to income ratio and the strategic decisions of store openings by Target.  While we don't have access to the performance data of existing stores, we can use demographic and economic information to identify regions with high potential for new store success based on regions that already have a Target store. By analyzing demographic and economic characteristics of different counties we can recommend prime locations for Target's expansion.\n",
        "\n",
        "Our key question is: Where should we open the next Target?\n",
        "\n",
        "We will be using 4 datasets for this project:\n",
        "\n",
        "1.   targets.csv - This dataset includes a record for Target locations currently in operation as of April 2017\n",
        "2.   debt_county.csv - This dataset includes a record of the household debt to income ration since 1999\n",
        "3.    pop_age_sex.csv - This dataset includes a record of population by age and sex per county\n",
        "4.    race_data.csv - This dataset includes a record of race per county\n",
        "\n",
        "Let's begin the cleaning process for target.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjGj11DhuF_X"
      },
      "source": [
        "## Target Data Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmXEvFmiVJ5i"
      },
      "outputs": [],
      "source": [
        "raw_target_data = pd.read_csv('/content/drive/MyDrive/Brainstation/Target Capstone /datasets/target.csv', encoding='latin1')\n",
        "raw_target_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROseJiSMzyfc"
      },
      "source": [
        "Here we can see the first 5 rows in our dataset and understand the types of data we will be working with, we see a combination of date, continues and categorical data. Let's check the shape of this data to understand how large the dataset is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6geZ2UEtWIf"
      },
      "outputs": [],
      "source": [
        "raw_target_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFo8NNLI0Eae"
      },
      "source": [
        "Here we see that there are 1829 rows/stores and 47 features. Let's check for duplicate values to ensure every row is unique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPNr4UQK0M1w"
      },
      "outputs": [],
      "source": [
        "raw_target_data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKvrTRNp0ayv"
      },
      "source": [
        "We see that there are no duplicated, but to be sure let's ensure there are no duplicate addresses. This ensures there are no duplicate stores even if the complete row is not completely the same.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWQXslzH00HU"
      },
      "outputs": [],
      "source": [
        "raw_target_data.duplicated(subset=['Address.AddressLine1']).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyWEfuoF1ZbS"
      },
      "source": [
        "Here we see again that there are no duplicates in terms of addresses."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's analyze the number of null values in our data set"
      ],
      "metadata": {
        "id": "ch8yJjYZbvmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_target_data.isnull().sum()"
      ],
      "metadata": {
        "id": "QzQ67ijMbvGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've identified null values in several columns, including 'Address.AddressLine2' (1771 nulls), 'SubTypeDescription' (1522 nulls), 'LocationMilestones.LastRemodelDate' (395 nulls), 'Capabilities' (12 nulls), and 'Market' (91 nulls). These nulls may reflect non-remodeled locations or a lack of additional capabilities. Further analysis is needed to determine the most appropriate handling of these null values."
      ],
      "metadata": {
        "id": "_K5huvmWcJ_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now assess which columns are impactful to our analysis of answering our key question: Where should we open a new Target location?\n",
        "\n",
        "Let's list out all of the columns in the dataset"
      ],
      "metadata": {
        "id": "SH89tcdBb4eK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Column Relevance"
      ],
      "metadata": {
        "id": "La7xQJlBaYGU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRsThO3s2x-E"
      },
      "outputs": [],
      "source": [
        "raw_target_data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GqvFA1e21ZH"
      },
      "source": [
        "In this section, we observe various fields related to store opening times. While these variables are noteworthy, they are not currently pertinent to our analysis and will therefore be excluded. The focus will shift to other relevant data, which I plan to explore in more detail later in the notebook.\n",
        "\n",
        "The fields concerning store opening times will now be removed from the current dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70mT_oH23unc"
      },
      "outputs": [],
      "source": [
        "# List of columns to drop\n",
        "columns_to_drop = [\n",
        "    'BeginTime.MF', 'Is24Hours.MF', 'IsOpen.MF', 'Summary.MF',\n",
        "    'ThruTime.MF', 'BeginTime.Sat', 'Is24Hours.Sat', 'IsOpen.Sat',\n",
        "    'Summary.Sat', 'ThruTime.Sat', 'BeginTime.Sun', 'Is24Hours.Sun',\n",
        "    'IsOpen.Sun', 'Summary.Sun', 'ThruTime.Sun',\n",
        "    'TimeZone.TimeZoneCode', 'TimeZone.TimeZoneDescription',\n",
        "    'TimeZone.TimeZoneOffset.OffsetCode', 'OperatingHours..timeFormat',\n",
        "    'TimeZone.TimeZoneOffset.OffsetHours'\n",
        "]\n",
        "\n",
        "# Drop the columns\n",
        "raw_target_data_drop = raw_target_data.drop(columns=columns_to_drop)\n",
        "\n",
        "#check for relevant columns\n",
        "raw_target_data_drop.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RtjS6MY80yq"
      },
      "source": [
        "With the removal of columns pertaining to store operating hours complete, our next step is to assess the remaining columns for their relevance to our analysis. Immediately, it becomes apparent that some features, such as phone numbers, fax numbers, and the 'recognition of daylight savings' status, are not pertinent to our objectives. As such, we will proceed to discard these irrelevant features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZzlIV5s9Tyb"
      },
      "outputs": [],
      "source": [
        "# List of columns to drop\n",
        "columns_to_drop2 = ['PhoneNumber', 'FaxNumber', 'IsDaylightSavingsTimeRecognized',\n",
        "                    'Address.IntersectionDescription','Address.CountryName','AlternateIdentifier.ID','ID']\n",
        "\n",
        "# Drop the columns\n",
        "raw_target_data_drop.drop(columns=columns_to_drop2, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kkWpuAp-Khn"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#Check Columns\n",
        "raw_target_data_drop.columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNryX4ux-cLi"
      },
      "source": [
        "Now that we're down to 24 columns, at first glance, they all seem relevant. However, to ensure a thorough analysis, we will examine each column individually. This detailed exploration will help us understand the relevance of each column in the context of our primary question: Where should we open the next Target location?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz3Gq6YGhk2c"
      },
      "source": [
        "### Column Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect each column to better understand what they contain let's start with name"
      ],
      "metadata": {
        "id": "807t6T5wbS50"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oOUptzeHW0S"
      },
      "outputs": [],
      "source": [
        "raw_target_data['Name']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like the name column contains the names of stores, this is somewhat redundant since we have addresses to identify the store."
      ],
      "metadata": {
        "id": "ke9p2ezHcV-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate the Address Line 2 column"
      ],
      "metadata": {
        "id": "wQiTxif5dnoK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YUX4noastgr"
      },
      "outputs": [],
      "source": [
        "#explore address.line2\n",
        "raw_target_data_drop['Address.AddressLine2'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRkGAWag2sLv"
      },
      "source": [
        "Here we see the Adress Line 2 data which primarily contains unit numbers. Given that we already have the main address details in a separate field, and their are many nulls due to many locations not having a line 2 in their address -- 'Address Line 2' appears to be of limited relevance to our analysis.\n",
        "\n",
        "We will drop this feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kQNMX-a3b-G"
      },
      "outputs": [],
      "source": [
        "#drop AddressLine2\n",
        "raw_target_data_drop1 = raw_target_data_drop.drop('Address.AddressLine2', axis=1)\n",
        "\n",
        "#check drop\n",
        "raw_target_data_drop1.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD5TlnuXZuGZ"
      },
      "source": [
        "Now we can explore the subtype description field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hXfvrGqngTQ"
      },
      "outputs": [],
      "source": [
        "#explore subtype description\n",
        "\n",
        "raw_target_data_drop1['SubTypeDescription'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlMGvf6aqFIY"
      },
      "source": [
        "This column offers potentially valuable insights, as it categorizes the type of Target store, such as 'SuperTarget', 'TargetExpress', or 'City'. To address the issue of missing (NaN) values, one approach could be to enhance the dataset through web scraping. However, for the sake of simplicity and immediate analysis, I will initially assume that these NaN entries represent standard Target stores. Accordingly, I'll map all NaN values in this column to 'Regular', enabling a more streamlined and comprehensive analysis of store types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaFHFrJ9ZuGa"
      },
      "outputs": [],
      "source": [
        "#replace all null values with 'Regular'\n",
        "raw_target_data_drop1['SubTypeDescription'].fillna('Regular',inplace = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKQst9bdZuGa"
      },
      "outputs": [],
      "source": [
        "#check for no null values\n",
        "raw_target_data_drop1['SubTypeDescription'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPYjE4HRBnFX"
      },
      "outputs": [],
      "source": [
        "#check for unique values\n",
        "raw_target_data_drop1['SubTypeDescription'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoyC-d1sBdNV"
      },
      "source": [
        "We now see that there are no nulls in the SubTypeDescription column and all NaN have been replaced with 'Regular'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0iax6Nfa06s"
      },
      "source": [
        "Now it's time to explore the X.locale column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0uVvXRP5dFY"
      },
      "outputs": [],
      "source": [
        "#explore X.Locale\n",
        "raw_target_data_drop1['X.locale'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG7My9ji5nhF"
      },
      "source": [
        "This column only contains 'en-US' meaning all locations are the same we can drop this column as it is redundant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNRbzKY05wGX"
      },
      "outputs": [],
      "source": [
        "#drop x.locale\n",
        "raw_target_data_drop2 = raw_target_data_drop1.drop('X.locale', axis=1)\n",
        "\n",
        "#check drop\n",
        "raw_target_data_drop2.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwxZIM46ZuGb"
      },
      "source": [
        "Now let's evaluate the LastRemodeled field.\n",
        "The 'LastRemodeled' field is potentially crucial, as it may indicate a store's success through Target's reinvestment, offering indirect insights into store performance in the absence of direct sales data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofTGMCAo9E41"
      },
      "outputs": [],
      "source": [
        "#explore last remodeled field\n",
        "raw_target_data_drop2['LocationMilestones.LastRemodelDate'].isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYTQ7v-HC_zA"
      },
      "source": [
        "We see that there are 395 nulls to handle. Because this field's nulls could mean that the Target wasnt remodeled we will create a new field with a binary indicator. Set it to 1 if 'LocationMilestones.LastRemodelDate' is not null and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MZDlevcELpP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a binary indicator column\n",
        "raw_target_data_drop2['Remodeled'] = pd.notna(raw_target_data['LocationMilestones.LastRemodelDate']).astype(int)\n",
        "\n",
        "#check new column\n",
        "raw_target_data_drop2[['LocationMilestones.LastRemodelDate','Remodeled']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUUPf-hqEzPP"
      },
      "source": [
        "We can now see the Remodeled column ther 0's where there are NaNs and 1's where there are dates.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN8y1xmm_OlI"
      },
      "source": [
        "Let's convert the open date and last remodeled date column to pandas datetime in order to be able to use it for analysis later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kiJ5wUm_p2M"
      },
      "outputs": [],
      "source": [
        "# Convert the columns to datetime format\n",
        "raw_target_data_drop2['LocationMilestones.OpenDate'] = pd.to_datetime(raw_target_data_drop2['LocationMilestones.OpenDate'])\n",
        "raw_target_data_drop2['LocationMilestones.LastRemodelDate'] = pd.to_datetime(raw_target_data_drop2['LocationMilestones.LastRemodelDate'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKvcldpi_2qD"
      },
      "outputs": [],
      "source": [
        "raw_target_data_drop2[['LocationMilestones.OpenDate','LocationMilestones.LastRemodelDate']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByOrm_y9_WdG"
      },
      "source": [
        "Now let's explore the Market Column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZu9VphEZuGb"
      },
      "outputs": [],
      "source": [
        "raw_target_data_drop2['Market'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fcge_uM2FTr_"
      },
      "source": [
        "I was unable to find any documentation on what this field means so I will drop it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKGHQgS6ZuGv"
      },
      "outputs": [],
      "source": [
        "#drop Market column\n",
        "raw_target_data_drop3 = raw_target_data_drop2.drop('Market', axis = 1)\n",
        "\n",
        "#check for clean data\n",
        "raw_target_data_drop3.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX2DL01nF4xn"
      },
      "source": [
        "Now let's explore 'All Capability'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3vbUIp8GCze"
      },
      "outputs": [],
      "source": [
        "raw_target_data_drop3['AllCapability']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqjloAJ7GOV5"
      },
      "source": [
        "We see that this field contains lists of additional features that a Target has such as a CVS, Cafe, Bakery etc. This could be useful for analysis. In order to process this I will create binary indicators for each unique value."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's handle the null values by filling them with an empty list"
      ],
      "metadata": {
        "id": "3DQcAEqQh7sL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Wq0DkD8IY7b"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Replace NaN values with an empty list\n",
        "raw_target_data_drop3['AllCapability'].fillna('[]', inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's remove unwanted characters such as commas and brackets then use a string method that splits each entry in the 'AllCapability' column into a list, using the comma as a delimiter."
      ],
      "metadata": {
        "id": "bIIIQ7d7iAkD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIKynkMP1tdr"
      },
      "outputs": [],
      "source": [
        "# Remove unwanted characters from the string and split the values into lists\n",
        "raw_target_data_drop3['AllCapability'] = raw_target_data_drop3['AllCapability'].str.replace(\"[\\[\\]']\", '', regex=True)\n",
        "raw_target_data_drop3['AllCapability'] = raw_target_data_drop3['AllCapability'].str.split(', ')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_target_data_drop3['AllCapability']"
      ],
      "metadata": {
        "id": "jQYzXiMIjZMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'AllCapability' column now comprises a series of capabilities, but they are not yet in a Python list format suitable for analysis. To address this, we will create a new column designed to store these capabilities as proper Python lists, facilitating further manipulation and analysis."
      ],
      "metadata": {
        "id": "qlLzwCTbjQvp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59qRvVh97yeC"
      },
      "outputs": [],
      "source": [
        "# Assuming raw_target_data_drop3['AllCapability'] contains lists of capabilities\n",
        "# Convert each list of capabilities into a string of comma-separated values\n",
        "raw_target_data_drop3['AllCapability_str'] = raw_target_data_drop3['AllCapability'].apply(lambda x: ','.join(x) if isinstance(x, list) else '')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the creation of a column containing properly formatted lists, we're now in a position to apply one-hot encoding to each capability. This can be achieved using the 'get_dummies' method, effectively transforming each unique capability into its own binary column."
      ],
      "metadata": {
        "id": "i9JWziHXk4Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now use `str.get_dummies` which works on comma-separated strings to create one-hot encoding\n",
        "capabilities_dummies = raw_target_data_drop3['AllCapability_str'].str.get_dummies(sep=',')\n",
        "\n",
        "# Join the one-hot encoded DataFrame with the original DataFrame\n",
        "raw_target_data_drop3 = raw_target_data_drop3.join(capabilities_dummies)\n",
        "\n",
        "# You can drop the intermediate 'AllCapability_str' if it's no longer needed\n",
        "raw_target_data_drop3.drop('AllCapability_str', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "ZN6j8u8ej8Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the one hot encoded features now added to the dataset we can drop the AllCapabilities column"
      ],
      "metadata": {
        "id": "BSw8pxc2lUXF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bksr9zyQ1yyB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Drop the original 'AllCapability' column\n",
        "raw_target_data_drop3.drop('AllCapability', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC0TbDSZzQW0"
      },
      "outputs": [],
      "source": [
        "raw_target_data_drop3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcyob-z4G50c"
      },
      "outputs": [],
      "source": [
        "raw_target_data_drop3.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Tl9xeW2ImMt"
      },
      "source": [
        "We now see each Capability hot-encoded as a binary values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbYaq-DfICHW"
      },
      "source": [
        "### Statistical Analysis\n",
        "\n",
        "Let's explore the statistical distribution and possible correlations in this dataset. We'll start with a heat map of all numerical columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzNiHxYk-pSc"
      },
      "outputs": [],
      "source": [
        "# Define column groups\n",
        "groups = [\n",
        "    ['Address.Latitude', 'Address.Longitude', 'Remodeled', 'Address.AddressLine1', 'Address.City', 'Address.County', 'Address.PostalCode'],\n",
        "    ['Address.Latitude', 'Address.Longitude', 'Remodeled', 'Name'],\n",
        "    ['Address.Latitude', 'Address.Longitude', 'Remodeled', 'Bakery', 'Beer', 'Cafe-Pizza', 'Café'],\n",
        "    ['Address.Latitude', 'Address.Longitude', 'Remodeled', 'CVS pharmacy', 'MinuteClinic', 'Optical', 'Photo Lab'],\n",
        "    ['Address.Latitude', 'Address.Longitude', 'Remodeled', 'Drive Up', 'Fresh Grocery', 'Starbucks']\n",
        "]\n",
        "\n",
        "#Loop through each group to compute and display correlation matrix\n",
        "for index, group in enumerate(groups, 1):\n",
        "    correlation_matrix = raw_target_data_drop3[group].corr()\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "    plt.title(f\"Correlation Matrix for Group {index}\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUkCdMdQg6S7"
      },
      "source": [
        "Upon examining the correlation matrix, we observe predominantly weak linear relationships between variables, hinting that they might largely function independently. This independence mitigates concerns about multicollinearity. However, these low correlations don't necessarily indicate an absence of relationships; non-linear interactions might be present but not reflected in the current metrics. While the initial impression suggests some variables might have lesser relevance due to their low correlation with the target, I will approach feature selection with a holistic view by exploring other types of relationships"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuG9gqHb42ao"
      },
      "source": [
        "###Histogram Analysis\n",
        "\n",
        "Let's now plot histograms for each of the numeric columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVuDTQcYh8o7"
      },
      "outputs": [],
      "source": [
        "numeric_raw = raw_target_data_drop3.select_dtypes(include=['number'])\n",
        "\n",
        "for col in numeric_raw.columns:\n",
        "  numeric_raw.loc[:,col].plot(kind='hist')\n",
        "  plt.title(col)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGfLrfTCRIJ8"
      },
      "source": [
        "###Analysis of Histograms\n",
        "\n",
        "**Latitude and Longitude:**\n",
        "The distribution of the latitude and longitude values appears to be normal. This suggests a fairly even distribution of Target stores across the regions under consideration. Visualizing these coordinates on a map or plotting them in a scatter plot could provide more granular insights about store distribution relative to geographical regions.\n",
        "\n",
        "**Remodeled Stores:**\n",
        "The data indicates a higher number of stores that have undergone remodeling compared to those that haven't. This is an intriguing observation, as it suggests that Target is keen on reinvesting in existing locations. It would be valuable to explore what types of neighborhoods or areas Target is choosing to reinvest in. Factors like neighborhood income, demographic changes, or local competition could influence such decisions.\n",
        "\n",
        "**Capabilities:**\n",
        "An initial glance at the histograms reveals that a significant number of stores lack certain capabilities. Understanding this trend could be crucial for strategic decisions. The absence of these capabilities might be indicative of older store models, regional preferences, or specific business strategies. Delving deeper into this data could help identify opportunities for store upgrades, features based on regional needs, or even potential expansion with newer store models in untapped regions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTkhIYzQf06T"
      },
      "source": [
        "###Scatter Plot Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEVyS79rfvmJ"
      },
      "outputs": [],
      "source": [
        "# Scatter plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(raw_target_data_drop3['Address.Longitude'], raw_target_data_drop3['Address.Latitude'], c='red', marker='o')\n",
        "plt.title(\"Distribution of Target Stores\")\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Of8Nec3gX9m"
      },
      "source": [
        "### Analysis of the Geographic Distribution of Target Stores:\n",
        "\n",
        "Upon plotting the latitude and longitude coordinates of each Target store, the scatter plot  shapes out a  silhouette resembling that of the USA. This geographic distribution provides us with several valuable insights:\n",
        "\n",
        "**Broad Presence:** The distribution indicates that Target has a wide-reaching presence across the continental United States. The scattering of stores from coast to coast suggests a national strategy in place, aiming to cater to a diverse range of demographics and regional preferences.\n",
        "\n",
        "**Density Variations:** While the scatter plot outlines the country, variations in the density of points indicate regions where Target has a stronger retail presence. Densely populated areas or economic hubs likely have a higher concentration of stores, signaling the company's strategic placement in locations with potentially higher foot traffic and purchasing power.\n",
        "\n",
        "**Potential Gaps:** Conversely, areas with fewer points might represent regions with fewer Target outlets. These could either be regions that are less densely populated, or they could signify potential markets that Target has yet to tap into fully.\n",
        "\n",
        "**Regional Strategies:** The distribution also prompts questions about regional strategies. For example, are there specific services or products that Target offers in coastal areas versus inland regions? Analyzing the store's offerings in conjunction with their locations could reveal tailored strategies for different parts of the country."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's view this data as a chloropleth"
      ],
      "metadata": {
        "id": "ms8tbukH3h_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    raw_target_data_drop3,\n",
        "    lat='Address.Latitude',\n",
        "    lon='Address.Longitude',\n",
        "    hover_name=raw_target_data_drop3['Address.City'].astype(str) + '<br>' + \\\n",
        "               raw_target_data_drop3['Address.County'].astype(str),\n",
        "    color_discrete_sequence=['red'],\n",
        "    zoom=4, # You can adjust the zoom level\n",
        "    height=300\n",
        ")\n",
        "\n",
        "fig.update_layout(mapbox_style=\"open-street-map\")\n",
        "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "KMzobcC5oMFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_target_data_drop3.head()\n"
      ],
      "metadata": {
        "id": "hAVxJHp-3zhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Having gained a comprehensive understanding of the Target dataset, and after diligently cleaning and examining each column, we are now ready to shift our focus to the Debt to Income Ratio data for further analysis."
      ],
      "metadata": {
        "id": "xAcjb7EWscX7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwEC_-VXGnr6"
      },
      "source": [
        "## Debt to Income Ratio Data\n",
        "\n",
        "Let's load in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KciOBbFUGl6x"
      },
      "outputs": [],
      "source": [
        "raw_debt_data = pd.read_csv('/content/drive/MyDrive/Brainstation/Target Capstone /datasets/household-debt-by-county.csv')\n",
        "raw_debt_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixXhLUQOKMmR"
      },
      "source": [
        "This dataset provides a snapshot of a specific economic metric across various counties from the first quarter of 1999. Each entry corresponds to a county, identified by its Federal Information Processing Standards (FIPS) code, such as 1001 for Autauga, Alabama. The \"low\" and \"high\" columns present a range for the debt to income ratio, offering insights into the economic variance within each county during each quarter of the year."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9R_adrRnNAb"
      },
      "source": [
        "Upon seeing this data we can begin to think of some ways in which we might merge this dataset with the Target dataset.\n",
        "\n",
        "In order to see this accurately we will need to merge the data not only in the right place (location), but the right time (date).\n",
        "\n",
        "**Merging on Location**\n",
        "In our household debt to income data we have a FIPS code, in order to use this we will need to convert these codes to match the Target dataset which has zipcode & county name. We will do this my using an intermediary dataset that will help us match the code to the county name & zipcode which is in the Target dataset.\n",
        "\n",
        "**Merging on Date**\n",
        "In our household debt to income data we are given Quarters rather than specfic dates. This means we will need to convert the Target dates into Quarters in order to merge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be4-z5YWGRAH"
      },
      "source": [
        "### Hold Debt Data Inspection\n",
        "\n",
        "Let's first clean and perform EDA on the household debt dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I--D6R5_G4iO"
      },
      "outputs": [],
      "source": [
        "raw_debt_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr7fZO4nG7Vq"
      },
      "source": [
        "Looking at the data we have the date which is the first date of the quarter, the year, the quarter and the area FIPS code. We also have low and high from my research lower debt to income ratio means\n",
        "\n",
        "**Low:** This value likely signifies the lower boundary or provides an estimate of the minimal debt carried by households within the area. When the low value is relatively small, it may imply that a portion of the population has debt levels that could be considered manageable within the scope of the dataset.\n",
        "\n",
        "**High:** In contrast, this value is presumed to reflect the upper boundary or an estimate of the greatest amount of debt held by households in the area. When the high value is substantial, it suggests the presence of a subset of the population whose debt levels reach the uppermost range captured by the data.\n",
        "\n",
        "Smaller amounts of household debt might suggest a lower likelihood of financial duress among households, while larger debt amounts may indicate a higher potential for financial stress. I'm curious to see how this relates to where Target stores are located.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIV5R9zJGQc4"
      },
      "outputs": [],
      "source": [
        "raw_debt_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh27Ym30GxDu"
      },
      "outputs": [],
      "source": [
        "raw_debt_data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDRmIoI0GtZ-"
      },
      "outputs": [],
      "source": [
        "raw_debt_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwFTzbsZD4XP"
      },
      "outputs": [],
      "source": [
        "raw_debt_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDgouPGk7eb6"
      },
      "source": [
        "We see a considerable amount of nulls in the high column, we also see that year and quarter are of the type int64 however we will need date time column for our analysis. We will create a datetime column and then look into the nulls.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdm8xIwEClOd"
      },
      "outputs": [],
      "source": [
        "# Convert 'year' and 'qtr' into datetime format\n",
        "raw_debt_data['date'] = pd.to_datetime(raw_debt_data['year'].astype(str) + 'Q' + raw_debt_data['qtr'].astype(str))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRb6VitNGCs6"
      },
      "source": [
        "Let's further investigate the nulls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vCpyCDN7-F7"
      },
      "outputs": [],
      "source": [
        "raw_debt_data.loc[raw_debt_data['high'].isnull()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKsWjaBr8dS-"
      },
      "source": [
        "We see that all of the nulls in high have 3.43 in the low this leads me to believe that this ratio may represent the upper limit of household debt as reported by the Federal Reserve. We will handle the high nulls by filling them with 3.43 (the low value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEzQaP5dAlKm"
      },
      "outputs": [],
      "source": [
        "raw_debt_data['high'].fillna(raw_debt_data['low'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UywTCKs6BU9M"
      },
      "outputs": [],
      "source": [
        "raw_debt_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMYc0xyPCGWS"
      },
      "source": [
        "### Statistical Analysis\n",
        "\n",
        "Now that we've cleaned the data. Let's do a quick statistical analysis on the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj6PpWwZVdBR"
      },
      "outputs": [],
      "source": [
        "statistical_summary = raw_debt_data.describe()\n",
        "statistical_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcgGcX3RWAcd"
      },
      "source": [
        "In summary, the dataset shows a wide spread of household debt across many counties over two decades. The debt levels, as indicated by the low and high values, show considerable variability but tend to be more densely packed at the lower end of the scale. There is no indication of significant outliers in the high values, as the maximum high is equal to the maximum low, suggesting that in some counties, the range of debt is not broad. The fact that the 75th percentile of high values is less than the maximum (3.43) suggests that only a small proportion of counties have debt levels at the upper extreme.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxkUet2CWUQP"
      },
      "source": [
        "Let's better understand the distribution of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj4_nRxxXasM"
      },
      "outputs": [],
      "source": [
        "numeric_debt_data = raw_debt_data.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "\n",
        "for col in numeric_debt_data.columns:\n",
        "  numeric_debt_data.loc[:,col].plot(kind='hist')\n",
        "  plt.title(col)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9YyHLbaYyHO"
      },
      "source": [
        "Here we can see the distribution of our data I'll focus mostly on the high and low columns.\n",
        "\n",
        "**Low** We see that we have data mostly distributed towards the middle and lower end of the scale with a peak ratio around 1.0\n",
        "\n",
        "**High** Here we also see that the distribution mostly packed on the low end. We also see a surge around 3.4 because of the nulls that we filled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiGQDJSeaY4o"
      },
      "source": [
        "### Trend Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm6S0zhyaqRt"
      },
      "outputs": [],
      "source": [
        "# Trend Analysis\n",
        "average_debt_over_time = raw_debt_data.groupby('date').agg({'low': 'mean', 'high': 'mean'}).reset_index()\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(average_debt_over_time['date'], average_debt_over_time['low'], label='Average Low Debt', color = 'black')\n",
        "plt.plot(average_debt_over_time['date'], average_debt_over_time['high'], label='Average High Debt',color = 'red')\n",
        "plt.title('Trend of Average Household Debt Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Average Debt')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGYH5P2kbEbw"
      },
      "source": [
        "The trend plot shows the average low and high household debt values over time. Both series appear to follow a similar trend, which is expected given that the high values were filled using the low values for missing entries.\n",
        "\n",
        "Although over time the trends are increasing we also see some seasonality may be occuring. Let's analyze to better understand the seasonality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8T0iN-rTbSx0"
      },
      "outputs": [],
      "source": [
        "raw_debt_data['year'] = raw_debt_data['date'].dt.year\n",
        "raw_debt_data['quarter'] = raw_debt_data['date'].dt.quarter\n",
        "seasonal_debt = raw_debt_data.groupby('quarter').agg({'low': 'mean', 'high': 'mean'}).reset_index()\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(seasonal_debt['quarter'], seasonal_debt['low'], marker='o', label='Average Low Debt')\n",
        "plt.plot(seasonal_debt['quarter'], seasonal_debt['high'], marker='o', label='Average High Debt')\n",
        "plt.title('Seasonal Trend of Average Household Debt')\n",
        "plt.xlabel('Quarter')\n",
        "plt.ylabel('Average Debt')\n",
        "plt.xticks([1, 2, 3, 4], ['Q1', 'Q2', 'Q3', 'Q4'])\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CCVh8ymcIOI"
      },
      "source": [
        "The seasonal trend plot for average household debt shows the mean low and high values for each quarter. There doesn't appear to be significant fluctuations between quarters, suggesting that there may not be strong seasonality present in this aspect of the data.\n",
        "\n",
        "Let's now search for any outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc1fL6UCb8SB"
      },
      "outputs": [],
      "source": [
        "# Outlier Detection\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.boxplot(raw_debt_data['low'], vert=False)\n",
        "plt.title('Boxplot of Low Household Debt Values')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot(raw_debt_data['high'], vert=False)\n",
        "plt.title('Boxplot of High Household Debt Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg7f2K6QcizI"
      },
      "source": [
        "The boxplots for the low and high household debt values indicate that there are some outliers present in both distributions. We also see that high has a higher variance than low.\n",
        "\n",
        "\n",
        "Now that both datasets are clean we can begin the process of merging them. At this time in the process I will save the merge for Sprint 3 this is because I am still working through the process of converting the FIPS and ZIP to County Names. However with the data I have now I am able to run models on the datasets separately.\n",
        "\n",
        "For Sprint 3 I will merge them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvxRsQCSjz0w"
      },
      "source": [
        "### Models\n",
        "\n",
        "At this time the FIPS codes will be enough to be able to identify each county. Let's do a time series model to better understand county household debt over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNda6TC3fa5H"
      },
      "source": [
        "### Time Series Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB8OsAE2mdVb"
      },
      "source": [
        "Let's start with aggregating our data by date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xh14Ovjel9G2"
      },
      "outputs": [],
      "source": [
        "debt_aggregated = raw_debt_data.groupby('date').agg({'low': 'mean', 'high': 'mean'}).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emu-iJvanC0b"
      },
      "outputs": [],
      "source": [
        "time_series_low = debt_aggregated.set_index('date')['low']\n",
        "time_series_high = debt_aggregated.set_index('date')['high']\n",
        "time_series= time_series_low + time_series_high"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7jnreZjnXZt"
      },
      "source": [
        "Let's decompose for the low column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90r8V52MnYLp"
      },
      "outputs": [],
      "source": [
        "# Decompose the time series to observe trend, seasonality, and residuals\n",
        "decomposition = seasonal_decompose(time_series_low, model='additive', period=4)  # Quarterly data, hence period=4\n",
        "decomposition.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXvP_I6QAVa6"
      },
      "source": [
        "Here we see that there is seasonality found in our data and overall we see a peak in household debt increasing around 2008 and then somewhat decreasing and then plateauing for years after."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3nIcbGenlRr"
      },
      "source": [
        "Let's decompose for high"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXlz7NJmnnqO"
      },
      "outputs": [],
      "source": [
        "# Decompose the time series to observe trend, seasonality, and residuals\n",
        "decomposition = seasonal_decompose(time_series_high, model='additive', period=4)  # Quarterly data, hence period=4\n",
        "decomposition.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st-er3D3BGl9"
      },
      "source": [
        "Here we see the same trend an increase in household debt around 2008 and then a drop and an evening out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zXL_ZbTqOG3"
      },
      "source": [
        "For our model we will use SARIMAX which is suitable for seasonal data like quarterly debt levels. First let's do a time series for the low debt column\n",
        "\n",
        "We'll split the data into train and test(80/20). We will model 1999 - 2018 and leave the rest for predictions. Let's start with the low column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IvFtqyBN3ME"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Group by both date and quarter, then aggregate\n",
        "debt_aggregated = raw_debt_data.groupby(['date', 'quarter']).agg({'low': 'mean', 'high': 'mean'}).reset_index()\n",
        "\n",
        "debt_aggregated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9pl5B8AqePT"
      },
      "outputs": [],
      "source": [
        "# Focus on 'low' debt for the analysis\n",
        "time_series = debt_aggregated.set_index('date')['low']\n",
        "\n",
        "split_point = debt_aggregated[debt_aggregated['date'].dt.year < 2018].index.max()\n",
        "# Now, let's find the index representing 80% of the data up to that split point\n",
        "split_index_80 = int(split_point * 0.8)\n",
        "# Split the data\n",
        "train = time_series.iloc[:split_index_80]\n",
        "test = time_series.iloc[split_index_80:split_point]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TvX1xMUagKc"
      },
      "outputs": [],
      "source": [
        "# Fit a SARIMAX model to the training data\n",
        "model = SARIMAX(train, order=(1, 0, 0), seasonal_order=(1, 1, 1, 4))\n",
        "results = model.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMK_Ve54a-hs"
      },
      "source": [
        "Now that we've run the model let's do some predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYJVBtCPrO1x"
      },
      "outputs": [],
      "source": [
        "# Predict from the beginning of 2018 to the end of 2023\n",
        "pred_start = '2018-01-01'\n",
        "pred_end = '2023-12-31'\n",
        "predictions = results.get_prediction(start=pd.to_datetime(pred_start), end=pd.to_datetime(pred_end), dynamic=True)\n",
        "predicted_mean = predictions.predicted_mean\n",
        "predicted_conf_int = predictions.conf_int()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSDgZ3CKbJN3"
      },
      "source": [
        "Let's plot and evaluate the predictions now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIrXhbgaAf3B"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the observed data, the fit, and the forecast for both training and test sets\n",
        "'''\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train, label='Training Data', color='blue')\n",
        "plt.plot(test, label='Test Data', color='orange')\n",
        "plt.plot(train_predictions, label='Predicted Training Data', color='green', linestyle='--')\n",
        "plt.plot(test_predictions, label='Predicted Test Data', color='red', linestyle='--')\n",
        "plt.title('Time Series Forecasting: Training and Test Data with Predictions')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Debt Value')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKJsCX5yYHxr"
      },
      "source": [
        "Let's calculate the MAE and MAPE to evaluate how well our model performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQa5rntW0paa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Assuming raw_debt_data has been loaded, preprocessed, and split into train and test sets\n",
        "\n",
        "# Fit the SARIMAX model on the training data\n",
        "model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 4))\n",
        "results = model.fit()\n",
        "\n",
        "# Predict the training set\n",
        "train_predictions = results.predict(start=train.index[0], end=train.index[-1])\n",
        "\n",
        "# Predict the test set\n",
        "test_predictions = results.predict(start=test.index[0], end=test.index[-1])\n",
        "\n",
        "# Calculate MAE and MAPE for the training set\n",
        "train_mae = mean_absolute_error(train, train_predictions)\n",
        "train_mape = np.mean(np.abs((train - train_predictions) / train)) * 100\n",
        "\n",
        "# Calculate MAE and MAPE for the test set\n",
        "test_mae = mean_absolute_error(test, test_predictions)\n",
        "test_mape = np.mean(np.abs((test - test_predictions) / test)) * 100\n",
        "\n",
        "# Print the results\n",
        "print(f'Training MAE: {train_mae}')\n",
        "print(f'Training MAPE: {train_mape}%')\n",
        "print(f'Test MAE: {test_mae}')\n",
        "print(f'Test MAPE: {test_mape}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrWuLwrEWIUl"
      },
      "source": [
        "\n",
        "\n",
        "Thes relatively low MAE on the training data indicates that the model's predictions are, on average, about 0.0483 units away from the actual values in the training set. This suggests a good fit on the training data.\n",
        "Training MAPE: 3.6504671403246816%\n",
        "\n",
        "The MAPE of approximately 3.65% on the training set implies that the model's predictions are off by 3.65% from the actual values, on average. This is a reasonably low error percentage, suggesting that the model has captured the underlying patterns in the training data quite well.\n",
        "Test MAE: 0.0551989235411686\n",
        "\n",
        "The MAE on the test data is slightly higher than on the training data but still remains low. An average deviation of 0.0552 suggests the model has managed to generalize well, though slightly less effectively than on the training data.\n",
        "Test MAPE: 3.411125803650962%\n",
        "\n",
        "The MAPE on the test set is marginally lower than on the training set, indicating a good level of prediction accuracy. A MAPE of around 3.41% is generally considered good in many applications, showing that the model's predictive capability holds up well on unseen data.\n",
        "\n",
        "###Interpretation:\n",
        "The model demonstrates a good fit and reasonable predictive accuracy, as evidenced by the low MAE and MAPE values on both the training and test sets.\n",
        "\n",
        "The slightly higher MAE on the test set compared to the training set is normal and suggests a modest amount of overfitting to the training data. However, the similar performance on both training and test sets suggests that the model is neither overfitting nor underfitting significantly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4AutZE6jMv6"
      },
      "source": [
        "### Next Steps\n",
        "\n",
        "\n",
        "As for next steps I will process an intermediate dataset to help me convert the FIPS code to county Names and merge the datasets together so that I will be able to analyze, compare, and predict to answer our key question: How can household debt, specifically the household debt-to-income ratio, inform Target's strategic decisions when opening new stores?\n",
        "\n",
        "\n",
        "Now that we've run our base line models.My next steps are to tease out the more intricate patterns in the data, especially the non-linear relationships and interactions between features, I intend to deploy the Random Forest algorithm. I'll also use K-means clustering to categorize similar data points and employ mapping techniques for a visual representation of data patterns.I also intend to play around with the parameters to see if there is a possibility of improving the model any further.\n",
        "\n",
        "Concluding the analysis, I'll make predictions and carve out a strategy. Using my models, I aspire to predict the most promising locations for Target stores. Based on the insights from these models, I'll forge actionable strategies, such as identifying the best regions for new stores and spotlighting areas that might undergo notable financial shifts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-ShS7sdk4DQ"
      },
      "source": [
        "#Sprint 3\n",
        "For Sprint 3 I will be completing the following:\n",
        "1.   Adding and cleaning additional demographic data\n",
        "2.   Merging all datasets as needed\n",
        "3.   Running a number of Machine Learning models\n",
        "4.   Evaluating the models\n",
        "5.   Comparing the models\n",
        "\n",
        "Let's begin with adding and cleaning additional datasets. There are many ways to understand a population and many factors that might make a county a profitable or not profitable location to place a target store.\n",
        "\n",
        "I was able to find data for the following:\n",
        "\n",
        "1. Age by county\n",
        "2. Race and Ethnicity by county\n",
        "3. Personal income by county\n",
        "4. Unemployment Status by county\n",
        "\n",
        "We will clean and perform EDA on these now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx600wn1vwsE"
      },
      "source": [
        "### Cleaning & EDA on Age/Sex by County Data (*)\n",
        "\n",
        "Let's begin by reading in the population by Age and Sex Counties dataset from StatsAmerica.org\n",
        "\n",
        "This data set contains population estimates by age and sex for the U.S., states, counties, metros, micros, and EDDs, from 2000 to 2019.\n",
        "\n",
        "Source: U.S. Census Bureau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNSyDJEVvp9v"
      },
      "outputs": [],
      "source": [
        "pop_age_sex = pd.read_csv('/content/drive/MyDrive/Brainstation/Target Capstone /datasets/PopulationbyAgeandSexCounties.csv')\n",
        "pop_age_sex.head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5aXyN7-eKg4"
      },
      "source": [
        "Let's see if this dataset is clean by searching for nulls and duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIx_0GaoeKEQ"
      },
      "outputs": [],
      "source": [
        "pop_age_sex.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWLdl5Z1eoC9"
      },
      "outputs": [],
      "source": [
        "pop_age_sex.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUpCEHvneqCT"
      },
      "source": [
        "We see that we have nulls in the Population growth column. For now we will drop this column since we already have raw population growth accounted for in each year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWYdg1MugMEO"
      },
      "outputs": [],
      "source": [
        "#pop_age_sex.drop('Population Growth', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK0P9ypUyL6h"
      },
      "source": [
        "Let's see the data types we have in this dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fdfpZuh8cE-"
      },
      "outputs": [],
      "source": [
        "pop_age_sex.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNU44moj9SJ2"
      },
      "source": [
        "As expected we see numerical data except the description fields. We will want to convert the Year column into a date time field.\n",
        "\n",
        "Let's convert the year and then check for nulls in this dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHn5Sk_GFHp_"
      },
      "outputs": [],
      "source": [
        "pop_age_sex['Year'] = pd.to_datetime(pop_age_sex['Year'], format='%Y')\n",
        "pop_age_sex.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvBfR8U4yKpC"
      },
      "outputs": [],
      "source": [
        "pop_age_sex.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vFruP65y1K9"
      },
      "source": [
        "There are no nulls in this dataset which is great! Let's check for any duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTu4owrxzFbC"
      },
      "outputs": [],
      "source": [
        "pop_age_sex.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "131owrxgzeq0"
      },
      "source": [
        "We also have no duplicates in this dataset. Let's better understand the distibution of this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOLO1r9S8Q7o"
      },
      "outputs": [],
      "source": [
        "pop_age_sex.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4CCqmET8i4V"
      },
      "outputs": [],
      "source": [
        "pop_age_sex.groupby('Year')['Total Population'].sum().plot(kind='line')\n",
        "plt.title('Total Population by Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Total Population')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1syIrORz_2K2"
      },
      "source": [
        "Here we see total population over time. We see an increasing trendline with a large surge in around 2010/2011 I will do more research into why that is.\n",
        "\n",
        "As for now lets understand the distribution of the population by age and sex group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np4x7_ghAdQy"
      },
      "outputs": [],
      "source": [
        "age_columns = ['Population 0-4', 'Population 5-17',\n",
        "       'Population 18-24', 'Population 25-44', 'Population 45-64',\n",
        "       'Population 65+', 'Population Under 18', 'Population 18-54',\n",
        "       'Population 55+']  # Add all relevant columns\n",
        "pop_age_sex[age_columns].sum().plot(kind='bar', color = 'red')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhTP5mzsBz5h"
      },
      "source": [
        "We see that the majority of the population is adult between 18 - 54 and when we drill deeper we see that there are slightly more 25-44 year olds than any other group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vESVcEcSDUJZ"
      },
      "source": [
        "Let's check out the percentage change of each state's population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7mLtJphBUbg"
      },
      "outputs": [],
      "source": [
        "# Calculate the percentage change in population for each state\n",
        "pop_age_sex['Population Growth'] = pop_age_sex.groupby('Statefips')['Total Population'].pct_change()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.barplot(x='Statefips', y='Population Growth', data=pop_age_sex)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Population Growth by State')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSD5U4t_DmwN"
      },
      "source": [
        "Here we see that State 15 (Hawaii) has the highest percent change in population this is pretty interesting but could be do to a smaller size in population impacting the percent change. Additions to the population might have a larger impact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l16AqdaI4jpK"
      },
      "source": [
        "### Cleaning & EDA on Personal Income by County Data (SKIP THIS SECTION)\n",
        "\n",
        "Now let's analyze the personal income by county data from StatsAmerica.org\n",
        "\n",
        "BEA Personal Income, Per Capita Income\n",
        "Source: U.S. Bureau of Economic Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8MIehIr4rpb"
      },
      "outputs": [],
      "source": [
        "personal_income = pd.read_csv('/content/drive/MyDrive/Brainstation/Target Capstone /datasets/Counties Personal Income.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jjhWHmi6tCe"
      },
      "source": [
        "Let's check for nulls and duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlHsfu0t6xXe"
      },
      "outputs": [],
      "source": [
        "personal_income.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbo8ae8w68wv"
      },
      "outputs": [],
      "source": [
        "personal_income.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PFtvvzv7DdJ"
      },
      "source": [
        "We have no nulls or duplicates in this dataset. Let's see what data types were working with\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qXa_NxAEtZt"
      },
      "outputs": [],
      "source": [
        "personal_income.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfbEQJblEw8i"
      },
      "source": [
        "Besides the description fields we have integer data. We will need to convert the Year column to a date time field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gind2i-nFwhH"
      },
      "outputs": [],
      "source": [
        "personal_income['Year'] = pd.to_datetime(personal_income['Year'], format=\"%Y\")\n",
        "personal_income.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge3-c-Y7HCtl"
      },
      "source": [
        "Let's now analyze the data a bit more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-tquuqiHOED"
      },
      "outputs": [],
      "source": [
        "personal_income.groupby('Year')['Data'].sum().plot(kind='line')\n",
        "plt.title('Data by Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1NOq3ybhQXj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPIMLwFyInES"
      },
      "source": [
        "## Cleaning & EDA on Race by County Data (*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPzoa8-LJTLh"
      },
      "outputs": [],
      "source": [
        "race_data=pd.read_csv('/content/drive/MyDrive/Brainstation/Target Capstone /datasets/PopulationbyRaceCounties.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJUc7rK7KK1l"
      },
      "outputs": [],
      "source": [
        "race_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d86g1dHKVdv"
      },
      "source": [
        "Let's check for nulls and duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOibBa-1KZVn"
      },
      "outputs": [],
      "source": [
        "race_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npp-vbzYKofP"
      },
      "outputs": [],
      "source": [
        "race_data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq4GTVn5KrHb"
      },
      "source": [
        "We see that we have a few nulls:\n",
        "\n",
        "* White Alone                              10\n",
        "* Black Alone                              10\n",
        "* American Indian or Alaskan Native        10\n",
        "* Asian Alone                              10\n",
        "* Hawaiian or Pacific Islander Alone    31920\n",
        "* Two or More Races                     31920\n",
        "* Not Hispanic                             10\n",
        "* Hispanic                                 10\n",
        "\n",
        "Let's take a closer look at these nulls so that we can decide what to do with them\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S2Kg7EMLMLc"
      },
      "outputs": [],
      "source": [
        "race_data.loc[race_data['White Alone'].isnull()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW5nJAYbML0S"
      },
      "source": [
        "When we take a closer look we see 10 years of data for Puerto Rico is null we can assume these are what is causing the 10 nulls for.\n",
        "* White Alone                              10\n",
        "* Black Alone                              10\n",
        "* American Indian or Alaskan Native        10\n",
        "* Asian Alone                              10\n",
        "* Not Hispanic                             10\n",
        "* Hispanic                                 10\n",
        "\n",
        "We will drop the Puerto Rico data since there isn't much value added in this analysis with the nulls. Let's evaluate the 31920 nulls in Pacific Islander and Two or more races.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MWOJ3RBPBcZ"
      },
      "outputs": [],
      "source": [
        "#Drop puerto rico rows\n",
        "\n",
        "race_data = race_data[race_data['Statefips'] != 72]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCnVEsUzNECS"
      },
      "outputs": [],
      "source": [
        "race_data.loc[race_data['Hawaiian or Pacific Islander Alone'].isnull()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG3SKECyNfZi"
      },
      "source": [
        "For this project we can assume for the Hawaiian or Pacific Islander Alone & Two or More Races there are 0 in that county. We will replace these null values with 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVfbgNQkOpCm"
      },
      "outputs": [],
      "source": [
        "race_data['Hawaiian or Pacific Islander Alone'] = race_data['Hawaiian or Pacific Islander Alone'].fillna(0)\n",
        "race_data['Two or More Races'] = race_data['Two or More Races'].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pag33BF-O2dX"
      },
      "outputs": [],
      "source": [
        "race_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiEpKpYKP3Y1"
      },
      "source": [
        "Let's do a bit of analysis on the data to better understand it.\n",
        "\n",
        "Let's look at population over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM-4ZgsSP9ut"
      },
      "outputs": [],
      "source": [
        "race_data.groupby('Year')['Total Population'].sum().plot(kind='line')\n",
        "plt.title('Total Population by Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Total Population')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtVuH8sdQzDC"
      },
      "source": [
        "As expected we see population increasing over time. Let's look at each race over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt4vQQzDRLzO"
      },
      "outputs": [],
      "source": [
        "race_columns = ['White Alone', 'Black Alone', 'American Indian or Alaskan Native', 'Asian Alone', 'Hawaiian or Pacific Islander Alone', 'Two or More Races', 'Not Hispanic', 'Hispanic']\n",
        "\n",
        "for col in race_columns:\n",
        "  race_data.groupby('Year')[col].sum().plot(kind='line')\n",
        "  plt.title(col)\n",
        "  plt.xlabel('Year')\n",
        "  plt.ylabel(col)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PqY7G1mSQbu"
      },
      "source": [
        "In general we see that the populations of each group are trending upwards as expected. In the year 1999/2000 we see a sharp decrease in population which I am unable to source the reason why. I'd like to believe it is because many people thought the world was going to end in Y2K -\n",
        "[link here](https://www.eonline.com/news/541614/remember-when-everyone-thought-the-world-was-going-to-end-with-y2k) However I have no proof of this so we will just accept the decline as is.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: make a bar chart of all races\n",
        "\n",
        "race_columns = ['White Alone', 'Black Alone', 'American Indian or Alaskan Native', 'Asian Alone', 'Hawaiian or Pacific Islander Alone', 'Two or More Races', 'Not Hispanic', 'Hispanic']\n",
        "race_data[race_columns].sum().plot(kind='bar', color = 'red')\n"
      ],
      "metadata": {
        "id": "oaofrWuk70i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcBmwjN4SOzO"
      },
      "source": [
        "## Cleaning & EDA on Unemployment by County (SKIP THIS SECTION)\n",
        "\n",
        "This data has information from 2000-2022\n",
        "\n",
        "Because this data is structured in a way that makes it challenging to analyze. I will not be merging this dataset to the final table for this sprint.\n",
        "\n",
        "Please move forward to the next section \"Location Data Processing\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaocAXRES7Du"
      },
      "outputs": [],
      "source": [
        "unemployment_data=pd.read_csv('/content/drive/MyDrive/Brainstation/Target Capstone /datasets/unemployment_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVpsoDppTEMO"
      },
      "outputs": [],
      "source": [
        "unemployment_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM7OhF_93ecr"
      },
      "outputs": [],
      "source": [
        "unemployment_data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52eDHEaMTcgO"
      },
      "source": [
        "We find that we have data from 2000-2022 which works well for our analysis. Let's check for nulls and duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgMfeDRITt2H"
      },
      "outputs": [],
      "source": [
        "unemployment_data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4Ky3uOuT75g"
      },
      "outputs": [],
      "source": [
        "unemployment_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE-iWSuPUHri"
      },
      "source": [
        "There are no duplicates but we find a number of nulls. Let's explore these nulls further starting with 'Med_HH_Income_Percent_of_State_Total_2021' since it has the highest amount."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD1Ye5nEUMqS"
      },
      "outputs": [],
      "source": [
        "unemployment_data.loc[unemployment_data['Med_HH_Income_Percent_of_State_Total_2021'].isna()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsvD4wUh5IUY"
      },
      "source": [
        "Interestingly enough we find that there are no columns similar to HH Median Income Percent of State Total in 2021 no other year has that attribute. For this reason I will explore dropping it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyqdnwXU6ns4"
      },
      "outputs": [],
      "source": [
        "unemployment_data = unemployment_data.drop(['Med_HH_Income_Percent_of_State_Total_2021'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy2JAlEX6rcq"
      },
      "source": [
        "Let's move on to our next null column Median Household income 2021. We find the same to be true there is no other column similar to this column so let's explore dropping it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJX4oIlY6uKS"
      },
      "outputs": [],
      "source": [
        "unemployment_data.loc[unemployment_data['Median_Household_Income_2021'].isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmYHDJo3H1OY"
      },
      "outputs": [],
      "source": [
        "unemployment_data = unemployment_data.drop(['Median_Household_Income_2021'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74rjL0USH485"
      },
      "outputs": [],
      "source": [
        "unemployment_data.loc[unemployment_data['Employed_2022'].isna()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbyP8MUvW8Zk"
      },
      "source": [
        "## Location Pre-processing\n",
        "\n",
        "\n",
        "Let's start the process of merging the data on location (FIP, ZIP, COUNTYNAME)\n",
        "\n",
        "\n",
        ". First let's read in our converter tables. It contains the county name, zipcode, state and FIPS code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J_gN4aMyDmS"
      },
      "outputs": [],
      "source": [
        "zip2fip = pd.read_csv('/content/drive/MyDrive/Brainstation/Target Capstone /datasets/zip2fips.csv')\n",
        "zip2fip.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On_SQA2DUpFJ"
      },
      "source": [
        "Let's ensure the data is clean by checking for nulls and duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1tfwic3NeaF"
      },
      "outputs": [],
      "source": [
        "zip2fip.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyXsCDOyU3Xf"
      },
      "outputs": [],
      "source": [
        "zip2fip.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YHetW6epsMQ"
      },
      "source": [
        "There are no duplicates or nulls.\n",
        "\n",
        "Let's now create a column called ZIP in the Target dataset by extracting it from the Formatted Address Column, this way we will be able to merge the datasets on ZIP and County name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTnNSOQfyLJW"
      },
      "outputs": [],
      "source": [
        "# extract ZIP code\n",
        "raw_target_data_drop3['ZIP'] = raw_target_data_drop3['Address.FormattedAddress'].str.extract(' (\\d{5})').astype(int)\n",
        "raw_target_data_drop3.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxM5-PnFqJYS"
      },
      "source": [
        "Let's also strip the word 'County' from the county name on our converter table since the Target dataset only has the name in it ('Calhoun' vs 'Calhoun County')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_wcssBOqJDy"
      },
      "outputs": [],
      "source": [
        "zip2fip['COUNTYNAME'] = zip2fip['COUNTYNAME'].str.replace(' County', '', regex=False)\n",
        "zip2fip.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIG04fCWrQq2"
      },
      "source": [
        "In order to merge the columns on ZIP and County we will need to change the names of the columns so that they're matching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XXnZ09QvFME"
      },
      "outputs": [],
      "source": [
        "raw_target_data_drop3.rename(columns={'Address.County': 'COUNTYNAME'}, inplace=True)\n",
        "raw_target_data_drop3.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DatotpCpZ21O"
      },
      "source": [
        "##Merge Target & Household Debt to Income\n",
        "\n",
        "Now that the data has been processed and analyzed,\n",
        "\n",
        "Let's begin with merging the Target dataset to our first demographic dataset-- household debt to income ratio table.\n",
        "\n",
        "In order to do so we will:\n",
        "-  Merge the Target data to a converter table\n",
        "-  Join the data on Quarter, Year, ZIP, and FIPS code\n",
        "-  Make a new column 'LastMilestoneEvent' that has the last date something occurred at the target (opening or remodel) in order to work with the nulls in the 'Remodel' column (we want to preserve the nulls)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0x1KgXAYQnX"
      },
      "source": [
        "Let's begin by creating a copy of the cleaned dataset and we will call is 'clean_target' this way we can always refer back to the original cleaned dataset if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9g8BiQkYPzK"
      },
      "outputs": [],
      "source": [
        "clean_target = raw_target_data_drop3.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8veNjuqdur8"
      },
      "source": [
        "Since we have nulls that we want to perserve in the 'LocationMilestones.LastRemodelDate' column we will create a new column 'LastMilestoneEvent' that contains the latest milestone event (for any Target that has not experienced a remodel the latest event will be the open date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C19sTUqDduUE"
      },
      "outputs": [],
      "source": [
        "clean_target['LastMilestoneEvent'] = clean_target['LocationMilestones.LastRemodelDate'].fillna(clean_target['LocationMilestones.OpenDate'])\n",
        "clean_target.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1PV3S3ytrHw"
      },
      "outputs": [],
      "source": [
        "clean_target.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IJ441NBfJJE"
      },
      "source": [
        "Our household debt data has a column for each quarter and we'll need to merge on it so we will make a new column for quarter of the year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jctovMJe3gv"
      },
      "outputs": [],
      "source": [
        "clean_target['Quarter'] = clean_target['LastMilestoneEvent'].dt.quarter\n",
        "clean_target.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVRTVXxJfZr3"
      },
      "source": [
        "Now let's add a Year  to join on since our household debt data also needs to be merged on 'Year'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8xviZOzfTzN"
      },
      "outputs": [],
      "source": [
        "clean_target['Year'] = clean_target['LastMilestoneEvent'].dt.year\n",
        "clean_target.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1NPewCEgSza"
      },
      "source": [
        "Let's check the new data types to ensure we have matching data types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p1ev7RvgSPM"
      },
      "outputs": [],
      "source": [
        "clean_target.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ6F5MfZgb70"
      },
      "outputs": [],
      "source": [
        "raw_debt_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgAGrL6oa5Q1"
      },
      "source": [
        "All data types needed to merge are the same (int 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdLWxRQ68PXv"
      },
      "source": [
        "Before we merge the data let's add a column isTarget that will help the ML model categorize locations with and without a Target store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqNtC3ri9npc"
      },
      "outputs": [],
      "source": [
        "clean_target['isTarget'] = 1\n",
        "clean_target.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orBV7IRgBw0l"
      },
      "source": [
        "Let's check for any nulls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv5ks1BwBzJh"
      },
      "outputs": [],
      "source": [
        "clean_target.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJGM9AjbB7pL"
      },
      "source": [
        "We see COUNTYNAME has 1 unexpected Null and Remodel has 395 expected nulls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiDRWhUHB7XC"
      },
      "outputs": [],
      "source": [
        "clean_target[clean_target['COUNTYNAME'].isnull()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPldS1qMCR1q"
      },
      "source": [
        "We find that the City is New York. Since this is in NYC the county name will also be New York let's confirm this with other Targets located in NYC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY1HfKlRDKTH"
      },
      "outputs": [],
      "source": [
        "clean_target[clean_target['Address.City'] == 'New York']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqIiqe-BEAwD"
      },
      "source": [
        "Let's fill the null county name with 'New York'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yipKTkaWCnD3"
      },
      "outputs": [],
      "source": [
        "clean_target['COUNTYNAME'].fillna('NY', inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVTzLpsQ0qL6"
      },
      "source": [
        "Let's also change 'Address.Subdivision' to 'State' so that we can merge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WYKrZzZ02WF"
      },
      "outputs": [],
      "source": [
        "clean_target.rename(columns={'Address.Subdivision': 'STATE'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDGN7i3tEIy3"
      },
      "source": [
        "Now let's merge Target data and the converter table. We will be using an outer join because we want to preserve all of the target locations as well as any locations without a target so that we can evaluate the differences in demographics.\n",
        "\n",
        "We will merge on the zipcode since that it is on the county level and I've noticed some inconsistencies in how the county name was written out so we will rely on zipcode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNlhY8biarut"
      },
      "outputs": [],
      "source": [
        "merged_data = pd.merge(clean_target, zip2fip, how='outer',on=['ZIP'])\n",
        "merged_data.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ1vmUe_cJJS"
      },
      "outputs": [],
      "source": [
        "merged_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU7KGRqKQhOc"
      },
      "source": [
        "We see that the shape of the data has become much larger which is expected.\n",
        "Let's check for Nulls, we expect to see many within the target data set since there are more counties than Target stores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1AS2OM7LtO1"
      },
      "outputs": [],
      "source": [
        "merged_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdnfE4Xx-4if"
      },
      "source": [
        "Let's dig deeper into some of the nulls that are unexpected\n",
        "*  STATE                                    1\n",
        "*  STCOUNTYFP                               1\n",
        "*  CLASSFP                                  1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKORsIkx2ymg"
      },
      "outputs": [],
      "source": [
        "merged_data[merged_data['STATE_y'].isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7u4YRDr3Cy2"
      },
      "outputs": [],
      "source": [
        "merged_data[merged_data['STCOUNTYFP'].isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q-_5lT23QKM"
      },
      "outputs": [],
      "source": [
        "merged_data[merged_data['CLASSFP'].isnull()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55xl955w3vr6"
      },
      "source": [
        "We see that the nulls are all the same row of data. Because we don't want to lose any of the target data we will fill this information in.\n",
        "\n",
        "We see that the state is WI, the county name is Waukesha, and the FIPS code is 55133. We actually won't need the ClassFP column so we can drop it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4qKephZ5bKo"
      },
      "outputs": [],
      "source": [
        "merged_data['STATE_y'].fillna('WI', inplace=True)\n",
        "merged_data['STCOUNTYFP'].fillna('55133', inplace=True)\n",
        "merged_data['COUNTYNAME_y'].fillna('Waukesha', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40QNtEYt5iRG"
      },
      "outputs": [],
      "source": [
        "merged_data.drop(columns=['CLASSFP'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFs_sWFL5lkc"
      },
      "outputs": [],
      "source": [
        "merged_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uhGIjMTdIMt"
      },
      "source": [
        "Next let's merge the House Hold Debt data to the Merged Target Data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogEpBifBhbTy"
      },
      "source": [
        "Let's change the name of the STCOUNTYFP to FIP inorder to merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQeKImjPhbCm"
      },
      "outputs": [],
      "source": [
        "merged_data.rename(columns={'STCOUNTYFP': 'FIP'}, inplace=True)\n",
        "merged_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN1ARpKj8aYL"
      },
      "source": [
        "Let's ensure the data types are the same so that we can merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deoClM_T8Kv7"
      },
      "outputs": [],
      "source": [
        "merged_data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S_ytKN08TEg"
      },
      "outputs": [],
      "source": [
        "raw_debt_data.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUXich568i49"
      },
      "source": [
        "We see that FIP has two different data types so let's convert the Target/Merged FIPs to int64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02ER8oGz8s3Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "merged_data['FIP'] = merged_data['FIP'].astype('int64')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx5z_j0Xhnv5"
      },
      "source": [
        "Let's change the household debt data columns to match with our other datasets so that we can merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HQQw-fPhmK2"
      },
      "outputs": [],
      "source": [
        "raw_debt_data.rename(columns={'area_fips': 'FIP'}, inplace=True)\n",
        "raw_debt_data.rename(columns={'year': 'Year'}, inplace=True)\n",
        "raw_debt_data.rename(columns={'quarter': 'Quarter'}, inplace=True)\n",
        "raw_debt_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUcaFMmbg7na"
      },
      "source": [
        "Due to time restraints I will perform and inner join on the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFoM_izXhGHh"
      },
      "outputs": [],
      "source": [
        "merged_data_2 = pd.merge(merged_data, raw_debt_data, how='inner', on=['FIP', 'Year', 'Quarter'])\n",
        "merged_data_2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cye_R33vic6w"
      },
      "outputs": [],
      "source": [
        "merged_data_2.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPM9Bgl0jCuE"
      },
      "outputs": [],
      "source": [
        "merged_data_2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvyBX5cCR9ad"
      },
      "source": [
        "## Merge Data by Age/Sex\n",
        "\n",
        "Now that we have merged the Target dataset and the household debt to income ratio data. Let's add additional datasets starting with Age and sex data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e1YcNA4SEXl"
      },
      "outputs": [],
      "source": [
        "pop_age_sex.head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2cRmN7uSfbR"
      },
      "source": [
        "Looking at the data in order to merge we will need to merge on the Year, the county, and the FIPS code. Something to note is that the household income data is per quarter while age and population is by year. I will assume that the QoQ data doesn't fluctuate too much and will be okay with the data being the same for each quarter as long as the year is aligned.\n",
        "\n",
        "In order to merge these are the steps that we will take:\n",
        "\n",
        "1. Pre-process fields to be able to merge (format 'Year', remove 'County' from County Name)\n",
        "2. Ensure the columns are the same name\n",
        "3. Merge!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLJKW41RmjCr"
      },
      "source": [
        "Let's begin with formatting the Year column currently it is a date time format and has the date for the 1st of the year so we can easily pull the Year from the Age by County data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvTImcuPmiM4"
      },
      "outputs": [],
      "source": [
        "pop_age_sex['Year'] = pop_age_sex['Year'].dt.year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIC_RKjknctF"
      },
      "outputs": [],
      "source": [
        "pop_age_sex.head(50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gF_3ePNm7Nq"
      },
      "source": [
        "Next let's remove the word 'county' from the county name and make a separate column for each state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmzIf7wpnR1A"
      },
      "outputs": [],
      "source": [
        "pop_age_sex['Location'] = pop_age_sex['Description'].str.replace('County', '')\n",
        "pop_age_sex.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA21hOVPoSjC"
      },
      "source": [
        "Now that we've made a new column with just the county name let's isolate the county name and state initials we'll need this for the merge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP2u4PcRofJB"
      },
      "outputs": [],
      "source": [
        "# Split the \"Column\" into two separate columns using the comma as the delimiter\n",
        "pop_age_sex[['County', 'State']] = pop_age_sex['Location'].str.split(',', expand=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_uHunHIl1od"
      },
      "outputs": [],
      "source": [
        "pop_age_sex.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd6fuuW2XD3S"
      },
      "source": [
        "Let's check if there are nulls after our new column was created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMQknMVjWf7f"
      },
      "outputs": [],
      "source": [
        "pop_age_sex.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m-oUR8EXsQv"
      },
      "source": [
        "We now see that there are 1,052 nulls in the dataset let's explore them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7w6ELW5Wtqd"
      },
      "outputs": [],
      "source": [
        "pop_age_sex.loc[pop_age_sex['State'].isnull()].head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk1BCepNY0xS"
      },
      "source": [
        "Looking at the nulls we see that the county column contains aggregate populations for the entire country or for the entire state for the year. We are aiming to analyze per county so we'll want to drop these rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kc3hJSGoaUXJ"
      },
      "outputs": [],
      "source": [
        "pop_age_sex.dropna(subset=['State'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXG7nHOqaxjp"
      },
      "outputs": [],
      "source": [
        "pop_age_sex.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmQdhBEbpnLy"
      },
      "source": [
        "Let's now drop the redundant columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_vgsq40pmoA"
      },
      "outputs": [],
      "source": [
        "pop_age_sex.drop(columns=['Description', 'Location','Statefips','Countyfips'], inplace=True)\n",
        "pop_age_sex.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z97FJ9vaqKQK"
      },
      "source": [
        "Now let's change the column names so that they match and we can merge. The columns we need are COUNTYNAME, FIP, STATE, YEAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHrFM0oHb4nA"
      },
      "outputs": [],
      "source": [
        "pop_age_sex.rename(columns={'County': 'COUNTYNAME'}, inplace=True)\n",
        "pop_age_sex.rename(columns={'State': 'STATE'}, inplace=True)\n",
        "pop_age_sex.rename(columns={'IBRC_Geo_ID': 'FIP'}, inplace=True)\n",
        "pop_age_sex.head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CgCGZQYbfbO"
      },
      "outputs": [],
      "source": [
        "master_data = pd.merge(merged_data_2, pop_age_sex, how='left', on=['FIP','Year'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF-8_rl0qQcU"
      },
      "outputs": [],
      "source": [
        "master_data.head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwCeCuzHuLKA"
      },
      "source": [
        "Now that we have successfully merged the datasets let ensure we have no nulls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKpus-q7q7X8"
      },
      "outputs": [],
      "source": [
        "master_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNIUcCxiuSbm"
      },
      "source": [
        "We see 3 nulls in a few columns. Let's explore them further"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQ6yAM20uSAV"
      },
      "outputs": [],
      "source": [
        "master_data.loc[master_data['COUNTYNAME_y'].isnull()].head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WlYwFzdvA0l"
      },
      "source": [
        "The data will nulls are from Targets that predate our demographic data. (1988-1999) we can drop these columns since most of our demographic data starts in 2000 with some as early as 1999."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBdhwxplvXQR"
      },
      "outputs": [],
      "source": [
        "master_data.dropna(subset=['COUNTYNAME_y'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJVpKa0Fvbg9"
      },
      "outputs": [],
      "source": [
        "master_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wcr5ZAePvaz5"
      },
      "source": [
        "Let's now move on to the next merge -- Personal Income."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFJNQtQGfmeg"
      },
      "source": [
        "## Merge Data by Race"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1SO0G0pwPpn"
      },
      "outputs": [],
      "source": [
        "race_data.tail(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY2KMR1_xC-x"
      },
      "source": [
        "Looking at the data we will need to merge on FIPS and Year\n",
        "\n",
        "In order to merge these are the steps that we will take:\n",
        "\n",
        "1. Ensure the columns are the same name\n",
        "2. Merge!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_fMLgJmxg3A"
      },
      "outputs": [],
      "source": [
        "race_data.rename(columns={'IBRC_Geo_ID': 'FIP'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4Hm6sSqxkd1"
      },
      "source": [
        "Now let's merge the columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R43QDqn4xnYL"
      },
      "outputs": [],
      "source": [
        "master_data = pd.merge(master_data, race_data, how='left', on=['FIP','Year'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y2fypaJxxV9"
      },
      "outputs": [],
      "source": [
        "master_data.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs4K9MuYx4Qr"
      },
      "source": [
        "Let's check for any nulls that mightve occured due to the merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BUMNn9gx8PX"
      },
      "outputs": [],
      "source": [
        "master_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SWv_o8Sx_yL"
      },
      "source": [
        "There are no nulls!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLn4QfgGx0kD"
      },
      "outputs": [],
      "source": [
        "master_data.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRuylLOnhGxN"
      },
      "source": [
        "## Statistical Analysis on Master Dataset\n",
        "\n",
        "Now that we have a master dataset we can perform some statistical analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8Va9qRSosVG"
      },
      "outputs": [],
      "source": [
        "master_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znqvDwE_1Dzm"
      },
      "source": [
        " We will now investigate the distribution of our columns and determine which, if any, of them need to be transformed in some way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxdKBvqo0kLO"
      },
      "outputs": [],
      "source": [
        "numeric_master_data = master_data.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "\n",
        "for col in numeric_master_data.columns:\n",
        "  numeric_master_data.loc[:,col].plot(kind='hist')\n",
        "  plt.axvline(numeric_master_data[col].mean(), label='Mean', c='red')\n",
        "  plt.axvline(numeric_master_data[col].median(), label='Median', c='blue')\n",
        "  plt.title(col)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVKUZyAp1aGY"
      },
      "source": [
        "We could either do these logarithmic (or other) transformations of our dataset in order to remove some of the skew that we see, HOWEVER, because we will be scaling this dataset, this transformation will be mostly lost when we do scaling and PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xrci8y8d1wmm"
      },
      "source": [
        "Now we will look at the correlations between our variables before we scale and transform them using dimensionality reduction techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70VxBy4B1hLN"
      },
      "outputs": [],
      "source": [
        "corr_df = master_data.corr()\n",
        "mask = np.triu(corr_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z52Nk-dM19Ue"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "sns.heatmap(corr_df, vmin=-1, vmax=1, cmap='coolwarm', annot=True, mask=mask) #annot is annotation\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYU7pGgW6fBW"
      },
      "source": [
        "From the correlation map we see that there is some multi-colinearity.This is important to note, however, since we will be using dimensionality reduction techniques, dealing with these issues of collinearity will be done automatically"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis testing"
      ],
      "metadata": {
        "id": "SM9cZbR5r6ix"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqYVBWc7TQ1e"
      },
      "source": [
        "## Scaling Master Data\n",
        "\n",
        "For our model we will need to scale our data. Let's first filter to have only numeric columns in our dataset inorder to scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6sD3sbjXn7L"
      },
      "outputs": [],
      "source": [
        "numeric_master_data = master_data.select_dtypes(include=['float64', 'int64'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMtrz86QUkyx"
      },
      "outputs": [],
      "source": [
        "numeric_master_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wfQJE25XqMF"
      },
      "source": [
        "Let's set our scalers a variable so that we can use them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOv50n2eXzy9"
      },
      "outputs": [],
      "source": [
        "ms = MinMaxScaler()\n",
        "ss = StandardScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1PPKlmxX51K"
      },
      "source": [
        "Now let's fit the data our chosen scalars."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuCEp25j0K2K"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_minmax_scaled = ms.fit_transform(numeric_master_data)\n",
        "data_standard_scaled = ss.fit_transform(numeric_master_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXagxsBcYRND"
      },
      "source": [
        "We see that these are numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbH7z7quYNoU"
      },
      "outputs": [],
      "source": [
        "type(data_minmax_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX9G1HzSYW_x"
      },
      "outputs": [],
      "source": [
        "type(data_standard_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj1aJgxjZ6CH"
      },
      "source": [
        "Let's convert the numpy arrays into pandas dataframes so that we can work with them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlmADX5pYqOE"
      },
      "outputs": [],
      "source": [
        "df_m = pd.DataFrame(data_minmax_scaled, columns=numeric_master_data.columns, index = numeric_master_data.index)\n",
        "df_s = pd.DataFrame(data_standard_scaled, columns=numeric_master_data.columns, index = numeric_master_data.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGPiI3FJqVkO"
      },
      "outputs": [],
      "source": [
        "df_m.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LLDx4FiDNKI"
      },
      "outputs": [],
      "source": [
        "np.isfinite(df_m).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKu4iIppAasn"
      },
      "outputs": [],
      "source": [
        "df_m.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeTRSIX5GU7o"
      },
      "outputs": [],
      "source": [
        "df_m.isna().sum().value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu1R401asVgR"
      },
      "source": [
        "## Dimensionality Reduction (SKIP THIS SECTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51VzRkf3eW0S"
      },
      "source": [
        "Now we want to reduce the dimensions of our datasets in order to plot the principle components against one another. We will create 6 different transformations based on 3 dimensionality reductions and the 2 scalings.\n",
        "\n",
        "We are going to do PCA on all of the data to better understand the similarities of all targets and demographics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E215G9xSetC1"
      },
      "outputs": [],
      "source": [
        "#PCA + MIN MAX\n",
        "pca_m = PCA(n_components=3)\n",
        "df_m_pca = pca_m.fit_transform(df_m)\n",
        "\n",
        "#PCA + STANDARD\n",
        "pca_s = PCA(n_components=3)\n",
        "df_s_pca = pca_s.fit_transform(df_s)\n",
        "\n",
        "#TSNE + MIN MAX\n",
        "tsne_m = TSNE(n_components = 3, perplexity = 15) #perplexity is a parameter\n",
        "df_m_tsne = tsne_m.fit_transform(df_m)\n",
        "\n",
        "#TSNE + STANDARD\n",
        "tsne_s = TSNE(n_components = 3, perplexity = 15) #perplexity is a parameter\n",
        "df_s_tsne = tsne_s.fit_transform(df_s)\n",
        "\n",
        "#UMAP + MINMAX\n",
        "umapM = UMAP(n_neighbors=10, n_components=3).fit_transform(df_m)\n",
        "\n",
        "#UMAP + STANDARD\n",
        "umapS = UMAP(n_neighbors=10, n_components=3).fit_transform(df_s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4-63NYfolKV"
      },
      "source": [
        "Now that we have our 6 dimentionality reduced dataframes, it's time to look at how they are scattered. Let's start with the PCA data.\n",
        "\n",
        "First we need to convert them from numpy data arrays to pandas dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOzWLRyTqI50"
      },
      "outputs": [],
      "source": [
        "df_m_pca = pd.DataFrame(df_m_pca, index=numeric_master_data.index)\n",
        "df_s_pca = pd.DataFrame(df_s_pca, index=numeric_master_data.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aykf9Wsjp6Z2"
      },
      "outputs": [],
      "source": [
        "#PCA MINMAX\n",
        "plt.figure()\n",
        "sns.pairplot(df_m_pca)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATz6FwU5qnIJ"
      },
      "outputs": [],
      "source": [
        "#PCA STANDARD\n",
        "plt.figure()\n",
        "sns.pairplot(df_s_pca)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-maFPbM4rJRT"
      },
      "source": [
        "Let's now work with the TSNE data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcA3dGTErOVY"
      },
      "outputs": [],
      "source": [
        "df_m_tsne = pd.DataFrame(df_m_tsne, index=numeric_master_data.index)\n",
        "df_s_tsne = pd.DataFrame(df_s_tsne, index=numeric_master_data.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY0NQu_urVWz"
      },
      "outputs": [],
      "source": [
        "#TSNE MINMAX\n",
        "sns.pairplot(df_m_tsne)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGSIk4zhrY6U"
      },
      "outputs": [],
      "source": [
        "#TSNE STANDARD\n",
        "sns.pairplot(df_s_tsne)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxGQ8yrrrc3M"
      },
      "source": [
        "Now let's do the same for UMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHzE_-M7rk98"
      },
      "outputs": [],
      "source": [
        "df_umapM = pd.DataFrame(umapM, index=numeric_master_data.index)\n",
        "df_umapS = pd.DataFrame(umapS, index=numeric_master_data.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Uw6pPndrfss"
      },
      "outputs": [],
      "source": [
        "#UMAP MINMAX\n",
        "sns.pairplot(df_umapM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29UwYEtSrswD"
      },
      "outputs": [],
      "source": [
        "#UMAP STANDARD\n",
        "sns.pairplot(df_umapS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUfM5LVzsQxQ"
      },
      "source": [
        "We will now look at 3d plots of these techniques to assess which is most separable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD5Ex9ZPse-H"
      },
      "outputs": [],
      "source": [
        "#PCA Minmax\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "ax.scatter(df_m_pca[0],df_m_pca[1],df_m_pca[2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O4k1Gpnsy17"
      },
      "outputs": [],
      "source": [
        "#PCA Standard\n",
        "plt.figure(figsize=(15,15))\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "ax.scatter(df_s_pca[2],df_s_pca[1],df_s_pca[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFa7YIPAs2SO"
      },
      "outputs": [],
      "source": [
        "#TSNE M\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "ax.scatter(df_m_tsne[0],df_m_tsne[1],df_m_tsne[2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ea_IgVmUs5dX"
      },
      "outputs": [],
      "source": [
        "#TSNE S\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "ax.scatter(df_s_tsne[2],df_s_tsne[1],df_s_tsne[0])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ8LpsObs8bp"
      },
      "outputs": [],
      "source": [
        "#UMAP M\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "ax.scatter(df_umapM[2],df_umapM[1],df_umapM[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQkOPbeLs_g2"
      },
      "outputs": [],
      "source": [
        "#UMAP S\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "ax.scatter(df_umapS[0],df_umapS[1],df_umapS[2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsUFtJ-dtdls"
      },
      "source": [
        "## Creating a New Dataset w/ isTarget Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOjHYq0_rWa8"
      },
      "source": [
        "We're going to do analysis on the just the population data and add an isTarget column we will use this as labels when clustering. We will attempt to see if there are any ripe conditions for a Target store.\n",
        "\n",
        "First we'll make a merged data set of just the population data ( population by age and sex, household debt data, and race data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4g5dfmiSgT2"
      },
      "outputs": [],
      "source": [
        "merged_demo_data = pd.merge(pop_age_sex,raw_debt_data, how='inner', on=['Year','FIP'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r57rz64FUKIq"
      },
      "outputs": [],
      "source": [
        "merged_demo2 = pd.merge(merged_demo_data,race_data, how ='inner', on=['Year','FIP'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_9IzivFUc9Q"
      },
      "outputs": [],
      "source": [
        "merged_demo2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTKV4VNir4y3"
      },
      "source": [
        "Let's check for nulls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCwHOWg3ViYf"
      },
      "outputs": [],
      "source": [
        "merged_demo2.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_dWoKtMr9yx"
      },
      "source": [
        "No Nulls but we do have some duplicates let's drop them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZSc1aUkWs42"
      },
      "source": [
        "Let's remove duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fi0BgqRfWvEo"
      },
      "outputs": [],
      "source": [
        "merged_demo2.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhV6REOGXK5C"
      },
      "outputs": [],
      "source": [
        "merged_demo2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hBqSF1gsFtA"
      },
      "source": [
        "In order to run analysis we want to take a snap shot of the most recent year in our data to make predictive analysis on \"Where should we open a Target?\"?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KroVenArUywC"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "max_year = merged_demo2['Year'].max()\n",
        "max_year\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgXZiV3LUukG"
      },
      "source": [
        "The latest year is 2019 Now let's filter only the most recent demographic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GwWELceVR3U"
      },
      "outputs": [],
      "source": [
        "filtered_2019 = merged_demo2[merged_demo2['Year'] == 2019]\n",
        "filtered_2019"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0G_6g69Ygu9"
      },
      "source": [
        "Now let's extract Quarter 1 so that we have a snap shot of whats going on. We will assume data from quarter 1 is for the year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKrIbwzXYrrM"
      },
      "outputs": [],
      "source": [
        "annual_data = filtered_2019[filtered_2019['qtr']== 1 ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCAVC2LQZcgy"
      },
      "outputs": [],
      "source": [
        "annual_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrwP4DpTZ0X7"
      },
      "source": [
        "Checking that the number of rows is the same as the number of unique values for FIP code ensures that we have no duplicate counties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYJoiZ1ZZl_d"
      },
      "outputs": [],
      "source": [
        "annual_data.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNP-airmaVyK"
      },
      "outputs": [],
      "source": [
        "master_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xADYZWlOcYYr"
      },
      "source": [
        "Now we want to filter the FIPS codes where there is a Target\n",
        "Then remove that from the dataset so that we can separate out the isTarget 0 vs 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLy6r4c-cXpd"
      },
      "outputs": [],
      "source": [
        "FIPS = master_data['FIP'].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxXChnOWefgr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "annual_data = annual_data[~annual_data['FIP'].isin(FIPS)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz_e2aRbetl0"
      },
      "outputs": [],
      "source": [
        "annual_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76tIaw4GfZtH"
      },
      "source": [
        "Now that we have all of the counties where there is no Target we can add isTarget = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCShPfLxfZWC"
      },
      "outputs": [],
      "source": [
        "annual_data['isTarget'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hukKTyW4gZih"
      },
      "outputs": [],
      "source": [
        "annual_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Otz9rCegmf-"
      },
      "outputs": [],
      "source": [
        "master_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc8aa9ohhKv4"
      },
      "source": [
        "Now let's join in historical population data where there is a target Let's update isTarget to 1 from our master dataset (contains Target & demo data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCPTcpE7hJG3"
      },
      "outputs": [],
      "source": [
        "master_data['isTarget'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhX7Fv0CCV1N"
      },
      "outputs": [],
      "source": [
        "master_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eieBK9oXh_rJ"
      },
      "source": [
        "Let's join on only the same columns (demographic data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqAc38NVh-zD"
      },
      "outputs": [],
      "source": [
        "common_columns = set(master_data.columns) & set(annual_data.columns)\n",
        "\n",
        "common_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqPum0ZaihZ7"
      },
      "outputs": [],
      "source": [
        "result = pd.merge(master_data[common_columns],annual_data[common_columns], how = 'outer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5xvW9XyjtAY"
      },
      "source": [
        "Now we have a dataset that contains all counties whether there is a target or not as well as a label indicating where each county falls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91eyvAUnjCy2"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XmjH44DtoQE"
      },
      "source": [
        "Let's check for nulls from our Join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4DtdYharWbx"
      },
      "outputs": [],
      "source": [
        "result.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVEktueWtsxS"
      },
      "source": [
        "We see that we have 5 nulls in a few columns let's take a closer look at them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpLbMCxzrc08"
      },
      "outputs": [],
      "source": [
        "result[result['STATE'].isnull() == True]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE75hIP2sZ0R"
      },
      "source": [
        "There are nulls because we don't have data dating back to 1999 for all columns. Let's drop these columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSXJgxDlsYlo"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "result = result.dropna(subset=['STATE'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_McNAl2s6XF"
      },
      "outputs": [],
      "source": [
        "result.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAhtVDty9wPm"
      },
      "source": [
        "## Test Run W/o Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiFQZCFL-8Ed"
      },
      "source": [
        "First let's separate X and Y and make a copy of the original dataframe/ drop redundant columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn6lI79J_EhB"
      },
      "outputs": [],
      "source": [
        "# making a copy of original dataframe and dropping problematic and redundant location columns since we have FIP codes\n",
        "x = result.copy()\n",
        "x = x.drop(columns=['isTarget', 'date', 'STATE', 'COUNTYNAME', 'Description','Year','FIP','Statefips','Countyfips','Quarter','qtr'])\n",
        "y = result['isTarget']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVpIvij2DnRe"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfHdYJQT_nMk"
      },
      "source": [
        "Drop date column since the model can't use it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOJ0b6ZDBO3Z"
      },
      "outputs": [],
      "source": [
        "x = x.astype(float)\n",
        "x.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRfxaEjH-UMG"
      },
      "source": [
        "First let's scale the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoEHkLqk-Xw_"
      },
      "outputs": [],
      "source": [
        "#min max x\n",
        "\n",
        "x_scaled = StandardScaler().fit_transform(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4udB3PvEqKij"
      },
      "source": [
        "Run a Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "094EVWSk96HA"
      },
      "outputs": [],
      "source": [
        "#run a logistic regression\n",
        "\n",
        "X = x_scaled\n",
        "y = y\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y )\n",
        "\n",
        "\n",
        "# Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "predict_all = model.predict(X)\n",
        "y_pred = model.predict(X_test)\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overall Accuracy (0.9315 or 93.15%): **This metric indicates that the model correctly predicted the class of the data points 93.15% of the time. This high accuracy suggests that the model is performing well in distinguishing between the two classes (0 and 1).\n",
        "\n",
        "**Class-Specific Analysis:**\n",
        "\n",
        "Class 0:\n",
        "Precision (0.89 or 89%): **bold text** Out of all instances predicted as class 0, 89% actually belong to class 0. This indicates a low false positive rate for class 0.\n",
        "\n",
        "**Recall (0.98 or 98%): **Out of all actual instances of class 0, 98% were correctly identified by the model. This indicates that the model is very effective at detecting class 0 instances.\n",
        "\n",
        "**F1-Score (0.93):** The harmonic mean of precision and recall for class 0 is 0.93, suggesting a good balance between precision and recall for this class.\\\n",
        "\n",
        "** Class 1\n",
        "Precision (0.98 or 98%)**:This high precision means that 98% of predictions made by the model for class 1 are correct, indicating a very low false positive rate.\n",
        "\n",
        "**Recall (0.88 or 88%):** This suggests that the model correctly identifies 88% of all actual class 1 instances. While still high, it's notably lower than the recall for class 0.\n",
        "\n",
        "**F1-Score (0.93): ** The F1-score for class 1 is also 0.93, showing a good balance between precision and recall, although the recall is somewhat lower compared to class 0."
      ],
      "metadata": {
        "id": "Umq7VZDGCclt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write X to csv\n",
        "\n",
        "x.to_csv('x_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "5M110EzkafMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lbv-OeEGtKKO"
      },
      "outputs": [],
      "source": [
        "type(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh4WXmxWHlng"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPKuimeQIxAF"
      },
      "source": [
        "**Top-Left Cell (True Negatives, TN):** 574 instances were correctly predicted as not belonging to the target class (class 0).\n",
        "\n",
        "**Top-Right Cell (False Positives, FP):** 7 instances were incorrectly predicted as belonging to the target class (class 0) when they did not.\n",
        "\n",
        "**Bottom-Left Cell (False Negatives, FN):** 22 instances were incorrectly predicted as not belonging to the target class (class 1) when they did.\n",
        "\n",
        "**Bottom-Right Cell (True Positives, TP):** 565 instances were correctly predicted as belonging to the target class (class 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtT6gNoWhps6"
      },
      "source": [
        "Let's dive deeper into the False Positives because we can use them to better understand where market opportunities may be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIKeLdDCla1Y"
      },
      "outputs": [],
      "source": [
        "#y_pred = pd.DataFrame(y_pred)\n",
        "y_test = pd.DataFrame(y_test)\n",
        "# X_test = pd.DataFrame(X_test)\n",
        "\n",
        "y_test['Predictions'] = y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xifr5QVbth92"
      },
      "outputs": [],
      "source": [
        "type(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_all = pd.DataFrame(predict_all)"
      ],
      "metadata": {
        "id": "FPqDm25rq1Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5ndij8mh0IE"
      },
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5FU9qWrt00E"
      },
      "outputs": [],
      "source": [
        "false_positives = y_test.loc[(y_test['Predictions'] == 1) & (y_test['isTarget'] == 0),:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qh5aFReuqyM"
      },
      "outputs": [],
      "source": [
        "market_opp = pd.merge(false_positives, result, how='left', left_index=True, right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiQFAt3OvbSu"
      },
      "outputs": [],
      "source": [
        "market_opp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit"
      ],
      "metadata": {
        "id": "lVxI3y7VtmPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#streamlit app data\n",
        "\n",
        "\n",
        "streamlit_data = X.copy()\n",
        "streamlit_data = pd.DataFrame(streamlit_data, columns=x.columns)\n",
        "streamlit_data['Predictions'] = predict_all\n",
        "streamlit_data['isTarget'] = y\n"
      ],
      "metadata": {
        "id": "vz28xoOJklsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit_data['Predictions'].value_counts()"
      ],
      "metadata": {
        "id": "p0KupjVzrCvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit_data['County'] = result['COUNTYNAME']"
      ],
      "metadata": {
        "id": "ROKX6fjntWGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit_data"
      ],
      "metadata": {
        "id": "vVhqiB7WvGYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit_data.to_csv('streamlit_data_final2.csv', index=False)"
      ],
      "metadata": {
        "id": "G620MuCapX0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SSyOlWG0kk3m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rooxaMBMweoY"
      },
      "outputs": [],
      "source": [
        "zip2fip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb5MnuaNv8qL"
      },
      "outputs": [],
      "source": [
        "# prompt: rename zip2fip stcountfy to FIP\n",
        "\n",
        "zip2fip.rename(columns={'STCOUNTYFP':'FIP'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1zLhOQGvo5l"
      },
      "outputs": [],
      "source": [
        "market_opp2 = pd.merge(market_opp, zip2fip, how='left', on='FIP')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEzuIvDPUdbb"
      },
      "outputs": [],
      "source": [
        "# prompt: get the coefficients from the logistic regression\n",
        "\n",
        "coeff = model.coef_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpm-47DnVh9s"
      },
      "outputs": [],
      "source": [
        "# Now you can use\n",
        "feature_names = x.columns\n",
        "\n",
        "# Extracting coefficients\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Creating a DataFrame for better visualization\n",
        "coefficients_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "\n",
        "# Displaying the DataFrame\n",
        "print(coefficients_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWMM8ev_XrUc"
      },
      "source": [
        "**Positive Coefficients:**\n",
        "\n",
        "Features like 'Total Population_x', 'Quarter', 'Female Population', etc., have positive coefficients. This means that as the value of these features increases, the log-odds of the target variable being true (or the probability of the positive class) also increases.\n",
        "\n",
        "Notably, features like 'Quarter', 'Population 18-24', 'Not Hispanic' have relatively high coefficients (above 3), indicating a stronger relationship with the target variable's likelihood.\n",
        "\n",
        "'Total Population_x' and 'Total Population_y' have the same coefficient, suggesting they might be duplicate or highly correlated features.\n",
        "Negative Coefficients:\n",
        "\n",
        "**Negative Coefficients**\n",
        "\n",
        "'high' and 'American Indian or Alaskan Native' have negative coefficients. An increase in these features is associated with a decrease in the likelihood of the target variable being true.\n",
        "\n",
        "The negative coefficient for 'high' is quite small (-0.052814), implying a relatively minor influence in decreasing the likelihood compared to the impact of 'American Indian or Alaskan Native' (-0.603116).\n",
        "Magnitude of Coefficients:\n",
        "\n",
        "Features like 'Quarter', 'Population 18-24', 'Not Hispanic', and 'White Alone' have some of the largest coefficients, implying they are significant predictors in this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPctiMu0YWur"
      },
      "outputs": [],
      "source": [
        "# prompt: plot the coefficients\n",
        "\n",
        "coefficients_df.sort_values('Coefficient', ascending=False).head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's graph them"
      ],
      "metadata": {
        "id": "yO1KkRrcDPCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Assuming coefficients_df is your DataFrame and it has columns 'Feature' and 'Coefficient'\n",
        "# Sort the DataFrame by the 'Coefficient' column in ascending order\n",
        "coefficients_df_sorted = coefficients_df.sort_values(by='Coefficient')\n",
        "\n",
        "# Now plot the sorted DataFrame\n",
        "coefficients_df_sorted.plot(x='Feature', y='Coefficient', kind='bar', figsize=(10, 6))\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Coefficients')\n",
        "plt.title('Coefficients of Logistic Regression Model')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KUsDaoOrDRH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcvUyfvMrMc9"
      },
      "source": [
        "Highest negative coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a1e2s9Nq10O"
      },
      "outputs": [],
      "source": [
        "\n",
        "coefficients_df.sort_values('Coefficient', ascending=True).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model to a file\n",
        "joblib.dump(model, 'logistic_regression_model.pkl')\n"
      ],
      "metadata": {
        "id": "NXZcrUpbdicu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mkQ1cwkrVIW"
      },
      "source": [
        "## Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZpZ1ISWC54x"
      },
      "outputs": [],
      "source": [
        "#run a random forest classifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Random Forest Classifier\n",
        "modelRF = RandomForestClassifier()\n",
        "modelRF.fit(X_train, y_train)\n",
        "y_pred = modelRF.predict(X_test)\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('Classification Report:', classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6arQlN5n1l_"
      },
      "source": [
        "## Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXRC2IYrn-W8"
      },
      "source": [
        "Now lets remove the isTarget column in order to cluster. We'll start with dimensionality reduction using umap and scaling our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvvLfVcArJMM"
      },
      "outputs": [],
      "source": [
        "\n",
        "result2 = result.drop(columns=['isTarget'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37NwSzRy4pFK"
      },
      "outputs": [],
      "source": [
        "result2.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApzrnLGSv_44"
      },
      "outputs": [],
      "source": [
        "result['isTarget'] = result['isTarget'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrt69EUjuH2j"
      },
      "source": [
        "We'll want to filter for only numeric columns in order to model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C893mRjpQX_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "numeric_result = result2.select_dtypes(include=['float64', 'int64'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0Z8rBsauMZV"
      },
      "source": [
        "Now let's scale our data. We'll use a minmax and standard scalar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjbkQiIYqIuM"
      },
      "outputs": [],
      "source": [
        "\n",
        "result_minmax_scaled = ms.fit_transform(numeric_result)\n",
        "result_standard_scaled = ss.fit_transform(numeric_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYVZgjmFuUAx"
      },
      "source": [
        "We can't forget to turn our numpy arrays into pandas dataframes or else we won't be able to use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07fHnhorn8ve"
      },
      "outputs": [],
      "source": [
        "results_m = pd.DataFrame(result_minmax_scaled, columns=numeric_result.columns, index = numeric_result.index)\n",
        "results_s = pd.DataFrame(result_standard_scaled, columns=numeric_result.columns, index = numeric_result.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmvuhsTiudcf"
      },
      "source": [
        "We'll use a series of differnt dimensionality reduction techniques to know which is the best one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSyRusNkn1GM"
      },
      "outputs": [],
      "source": [
        "#UMAP + MINMAX\n",
        "result_umapM = UMAP(n_neighbors=10, n_components=3).fit_transform(results_m)\n",
        "\n",
        "#UMAP + STANDARD\n",
        "result_umapS = UMAP(n_neighbors=10, n_components=3).fit_transform(results_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSrOzvqnxOJo"
      },
      "outputs": [],
      "source": [
        "df_result_umapM = pd.DataFrame(result_umapM, index=numeric_result.index)\n",
        "df_result_umapS = pd.DataFrame(result_umapS, index=numeric_result.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKT6Z7135i03"
      },
      "outputs": [],
      "source": [
        "df_result_umapS.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izTrGZW4y12E"
      },
      "outputs": [],
      "source": [
        "#PCA + MIN MAX\n",
        "pca_m = PCA(n_components=3)\n",
        "result_m_pca = pca_m.fit_transform(results_m)\n",
        "\n",
        "#PCA + STANDARD\n",
        "pca_s = PCA(n_components=3)\n",
        "result_s_pca = pca_s.fit_transform(results_s)\n",
        "\n",
        "#TSNE + MIN MAX\n",
        "tsne_m = TSNE(n_components = 3, perplexity = 15) #perplexity is a parameter\n",
        "result_m_tsne = tsne_m.fit_transform(results_m)\n",
        "\n",
        "#TSNE + STANDARD\n",
        "tsne_s = TSNE(n_components = 3, perplexity = 15) #perplexity is a parameter\n",
        "result_s_tsne = tsne_s.fit_transform(results_s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OXzvL44zax8"
      },
      "outputs": [],
      "source": [
        "df_result_m_pca = pd.DataFrame(result_m_pca, index=numeric_result.index)\n",
        "df_result_s_pca = pd.DataFrame(result_s_pca, index=numeric_result.index)\n",
        "\n",
        "df_result_m_tsne = pd.DataFrame(result_m_tsne, index=numeric_result.index)\n",
        "df_result_s_tsne = pd.DataFrame(result_s_tsne, index=numeric_result.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ccrh-_WxzXE"
      },
      "source": [
        "We'll add the Target columns in for indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsrBZ15PvOZ_"
      },
      "outputs": [],
      "source": [
        "df_result_umapM['isTarget'] = result['isTarget']\n",
        "df_result_umapS['isTarget'] = result['isTarget']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ_eh7YD0ZD6"
      },
      "outputs": [],
      "source": [
        "df_result_m_pca['isTarget'] = result['isTarget']\n",
        "df_result_s_pca['isTarget'] = result['isTarget']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsoMipv20aun"
      },
      "outputs": [],
      "source": [
        "df_result_m_tsne['isTarget'] = result['isTarget']\n",
        "df_result_s_tsne['isTarget'] = result['isTarget']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRSdSp1nx6Nb"
      },
      "source": [
        "Now we'll drop the columns so that they dont sway our clustering results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0S57c2O6b61"
      },
      "outputs": [],
      "source": [
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzUNLbkG6eBv"
      },
      "source": [
        "### Identify which dataframe needs to be fed to the model from below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI1RGliy2-by"
      },
      "outputs": [],
      "source": [
        "# drop istarget\n",
        "\n",
        "df_result_umapM_drop = df_result_umapM.drop(columns=['isTarget'], inplace=False)\n",
        "df_result_umapS_drop = df_result_umapS.drop(columns=['isTarget'], inplace=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwrTEEGy0xkT"
      },
      "outputs": [],
      "source": [
        "df_result_m_pca = df_result_umapM.drop(columns=['isTarget'], inplace=False)\n",
        "df_result_s_pca = df_result_umapS.drop(columns=['isTarget'], inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63AyhSFc0yPB"
      },
      "outputs": [],
      "source": [
        "df_result_m_tsne = result_m_tsne.drop(columns=['isTarget'], inplace=False)\n",
        "df_result_s_tsne = result_m_tsne.drop(columns=['isTarget'], inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL1QilDz6OsI"
      },
      "outputs": [],
      "source": [
        "df_result_s_tsne.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRcn24rHx_xY"
      },
      "source": [
        "Now let's take a look at our results. We'll color based on if there is a Target (isTarget column) We'll look at UMAP first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff4OumCqu2Zf"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.scatter(df_result_umapM[2],df_result_umapM[1],df_result_umapM[0], c=df_result_umapM['isTarget'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO7FR9ynu2Nz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.scatter(df_result_umapS[2],df_result_umapS[1],df_result_umapS[0], c=df_result_umapS['isTarget'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhIX9lP51I37"
      },
      "source": [
        "Now we'll look at PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TW31nL-1Iin"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.scatter(df_result_m_pca[2],df_result_m_pca[1],df_result_m_pca[0], c=result['isTarget'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFkhYMeb1NBc"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.scatter(df_result_s_pca[2],df_result_s_pca[1],df_result_s_pca[0], c=result['isTarget'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKhUqSAX1NvK"
      },
      "source": [
        "Now we'll look at tSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRzJdb1I1Qvp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.scatter(df_result_m_tsne[2],df_result_m_tsne[1],df_result_m_tsne[0], c=result['isTarget'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycyIf83y1RqA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.scatter(df_result_s_tsne[2],df_result_s_tsne[1],df_result_s_tsne[0], c=result['isTarget'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHoxIN4_HZk9"
      },
      "source": [
        "In general we see that there is not great clustering no matter the dimensionality reduction. Let's see what we can do with a clustering algorithm K-Means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJcC4SqbOTUm"
      },
      "source": [
        "## Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLk9n99FyInw"
      },
      "source": [
        "Now let's run a clustering algorithm using the reduced dataset to see how the model would cluster any similarities and differences. First we'll run it using the umap minmax scaled data. We will use 2 clusters due to the fact that we want to predict if there is/should be a target or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtoTzuOW1eRX"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "# Create a list to store the silhouette scores for each value of n_clusters\n",
        "silhouette_scores_results = []\n",
        "\n",
        "# For each value of n_clusters, fit a KMeans model and calculate the silhouette score\n",
        "k2means = KMeans(n_clusters=2)\n",
        "k2means.fit(df_result_umapM_drop)\n",
        "cluster_labels = k2means.fit_predict(df_result_umapM_drop)\n",
        "silhouette_scores_results.append(silhouette_score(df_result_umapM_drop, k2means.labels_))\n",
        "\n",
        "# Find the value of n_clusters that maximizes the silhouette score\n",
        "best_n_clusters = np.argmax(silhouette_scores_results)\n",
        "\n",
        "\n",
        "# Print the silhouette score for the best value of n_clusters\n",
        "print(\"Silhouette score for the best value of n_clusters:\", silhouette_scores_results[best_n_clusters])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jovNBI1YykEg"
      },
      "source": [
        "The silhouette score is not the best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIQGg2fLym1T"
      },
      "source": [
        "Let's view the clusters colored by cluster labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LujLVx75LnG"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.scatter(df_result_umapS[0],df_result_umapS[1],df_result_umapS[2], c= cluster_labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQwBswnd5aP-"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.scatter(df_result_umapM[0],df_result_umapM[1],df_result_umapM[2], c= cluster_labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXkfZbZ9IjIh"
      },
      "source": [
        "Here we see great separation between the two categories when using Kmeans clustering for PCA min max scaled data however not much cohesion. In the interest of time we'll use this configuration of the data to run further analysis.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QD_8OtowPL0"
      },
      "outputs": [],
      "source": [
        "df_result_umapM.columns = df_result_umapM.columns.astype(str)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo1m3XSI5BNf"
      },
      "source": [
        "## Decision Tree\n",
        "\n",
        "Let's better understand our clusters using a decision tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec5vqbeDOjTQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cluster_labels = cluster_labels.astype(str)\n",
        "\n",
        "df_result_umapM.columns = df_result_umapM.columns.astype(str)\n",
        "#df_result_umapM_drop2 = df_result_umapM.drop('isTarget')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AODI210kxPYC"
      },
      "outputs": [],
      "source": [
        "df_result_umapM.columns.drop('isTarget')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrSWuqxpwzJr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Assuming df_result_umapM is your feature set and cluster_labels are the cluster labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_result_umapM, cluster_labels, test_size=0.25)\n",
        "\n",
        "# Create a decision tree classifier\n",
        "classifier = DecisionTreeClassifier()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Visualize the tree\n",
        "plt.figure(figsize=(20,10))\n",
        "tree.plot_tree(classifier, filled=True, feature_names=df_result_umapM.columns)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYZm9m7v-8iw"
      },
      "source": [
        "Let's also run a decision tree using isTarget as our Y variable to better understand the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQe5D4fu_6Qd"
      },
      "source": [
        "This probably should've been done first but better late than never. Let's do a Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPeiahqEWD4C"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GpG145-1GyQ"
      },
      "outputs": [],
      "source": [
        "# prompt: convert to a data frame\n",
        "df_test = pd.DataFrame(result_minmax_scaled)\n",
        "df_test.get_feature_names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcfaVlXeEcVX"
      },
      "source": [
        "Let's first try it on scaled (min max data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoQDi1FrAvY7"
      },
      "outputs": [],
      "source": [
        "# Prepare the data\n",
        "X = result_minmax_scaled # Features\n",
        "y = result['isTarget']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnLo9IXTEl2-"
      },
      "source": [
        "The logistical regression works perfectly. This might mean it's over fit we can run a confusion matrix to further evaluate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQC2RsLkFZXn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assuming y_test are the true labels and y_pred are the model's predictions\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyi-Qfi1Fr13"
      },
      "source": [
        "**Top-Left Cell (True Negatives, TN):** 466 instances were correctly predicted as not belonging to the target class (class 0).\n",
        "\n",
        "**Top-Right Cell (False Positives, FP):** 0 instances were incorrectly predicted as belonging to the target class (class 0) when they did not.\n",
        "\n",
        "**Bottom-Left Cell (False Negatives, FN):** 2 instances were incorrectly predicted as not belonging to the target class (class 1) when they did.\n",
        "\n",
        "**Bottom-Right Cell (True Positives, TP):** 467 instances were correctly predicted as belonging to the target class (class 1).\n",
        "\n",
        "##### Key takeaways:\n",
        "\n",
        "H**igh Sensitivity/Recall for Class 1:** The model successfully identified 467 out of 469 actual positive instances (class 1), which is nearly perfect.\n",
        "\n",
        "**Perfect Specificity for Class 0:** All 466 actual negative instances (class 0) were correctly identified. There were no false positives, which is remarkable.\n",
        "\n",
        "**Precision for Class 1:** Since there are no false positives, precision for class 1 is essentially perfect.\n",
        "\n",
        "**Minimal False Negatives:**Only 2 instances that should have been classified as class 1 were missed. This is very low, suggesting the model is quite reliable in its predictions.\n",
        "\n",
        "**Overall Accuracy:** Considering the total number of predictions (935), the model has an accuracy of [(466+467)/935] ≈ 99.79%, aligning with the high accuracy rate previously mentioned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJzQ0uEdHUy0"
      },
      "source": [
        "Let's better understand the coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYLbzh1oH1s1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "X_train_df = pd.DataFrame(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la4lv4nXHwEp"
      },
      "outputs": [],
      "source": [
        "# Extracting coefficients\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Assuming X_train has column names, which are your feature names\n",
        "feature_names = X_train_df.columns\n",
        "\n",
        "# Creating a DataFrame for easier visualization\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "\n",
        "\n",
        "\n",
        "# Displaying the DataFrame\n",
        "print(feature_importance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ovDucky3Ata"
      },
      "outputs": [],
      "source": [
        "print(result.columns)\n",
        "\n",
        "feature_importance2 = pd.Series(classifier.feature_importances_, index=result.columns)\n",
        "feature_importance2.nlargest(20).plot(kind='barh')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kam0zwNXJHDJ"
      },
      "outputs": [],
      "source": [
        "X_train_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-ELR53LE1Pg"
      },
      "source": [
        "Let's also run a logistical regression on standard scaled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN2qbAQUEbMw"
      },
      "outputs": [],
      "source": [
        "# Prepare the data\n",
        "X2 = result_standard_scaled # Features\n",
        "y2 = result['isTarget']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X2_train, y2_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y2_pred = model.predict(X2_test)\n",
        "accuracy2 = accuracy_score(y2_test, y2_pred)\n",
        "print(\"Accuracy:\", accuracy2)\n",
        "print(\"Classification Report:\\n\", classification_report(y2_test, y2_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBEctoSVFHS5"
      },
      "source": [
        "This works 100% of the time we will also do a confustion matrix to better understand our results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yfx2bhBwGIn6"
      },
      "outputs": [],
      "source": [
        "# Assuming y_test are the true labels and y_pred are the model's predictions\n",
        "cm2 = confusion_matrix(y2_test, y2_pred)\n",
        "\n",
        "print(cm2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdG9HU1DGhb5"
      },
      "source": [
        "Same analysis as above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1SS2MFLWK6z"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqX6d8kLHIAW"
      },
      "source": [
        "In conclusion... always do a logistical regression first it can save you alot of work.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}